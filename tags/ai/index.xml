<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on </title>
    <link>https://1rvinn.github.io/tags/ai/</link>
    <description>Recent content in Ai on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 08 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://1rvinn.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ai sec intern</title>
      <link>https://1rvinn.github.io/crude/ai_sec/</link>
      <pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://1rvinn.github.io/crude/ai_sec/</guid>
      <description>&lt;p&gt;so i&amp;rsquo;ve finally started an ai security intern. will keep on updating my findings and learnings here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;worked on llmguard [2025-08-11]&lt;/li&gt;
&lt;li&gt;working on promptfoo [2025-08-13]*&lt;/li&gt;
&lt;/ul&gt;</description>
      <content>&lt;p&gt;so i&amp;rsquo;ve finally started an ai security intern. will keep on updating my findings and learnings here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;worked on llmguard [2025-08-11]&lt;/li&gt;
&lt;li&gt;working on promptfoo [2025-08-13]*&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>bubble</title>
      <link>https://1rvinn.github.io/build/bubble/</link>
      <pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://1rvinn.github.io/build/bubble/</guid>
      <description>&lt;hr&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;h3 style=&#34;color: #23affd;&#34;&gt; // the final thing &lt;/h3&gt;
&lt;p&gt;check it out here: &lt;a href=&#34;https://github.com/1rvinn/bubble_v2&#34;&gt;https://github.com/1rvinn/bubble_v2&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/WdP8bOORbTs&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;br&gt;
    &lt;h3 style=&#34;color: #23affd;&#34;&gt; // building it out - the journey&lt;/h3&gt;
&lt;/div&gt;
for the gemma 3n hackathon by google deepmind, i intend on making a local ai based screen overlay helper that allows people to call it to command wherever and whenever they face an issue in their day to day tasks. 
&lt;p&gt;the vision is pretty similar to google’s gemini live screen streaming, but with an added layer of ui - a screen overlay that displays the key troubleshooting steps/guide along with proper highlighting of the important elements to be accessed in the process.&lt;/p&gt;</description>
      <content>&lt;hr&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;h3 style=&#34;color: #23affd;&#34;&gt; // the final thing &lt;/h3&gt;
&lt;p&gt;check it out here: &lt;a href=&#34;https://github.com/1rvinn/bubble_v2&#34;&gt;https://github.com/1rvinn/bubble_v2&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/WdP8bOORbTs&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;br&gt;
    &lt;h3 style=&#34;color: #23affd;&#34;&gt; // building it out - the journey&lt;/h3&gt;
&lt;/div&gt;
for the gemma 3n hackathon by google deepmind, i intend on making a local ai based screen overlay helper that allows people to call it to command wherever and whenever they face an issue in their day to day tasks. 
&lt;p&gt;the vision is pretty similar to google’s gemini live screen streaming, but with an added layer of ui - a screen overlay that displays the key troubleshooting steps/guide along with proper highlighting of the important elements to be accessed in the process.&lt;/p&gt;
&lt;p&gt;what this possibly could look like is people in photoshop, autocad or matlab, being stuck with finding a particular feature, unable to move ahead. now, instead of laboriously looking for a solution across google, youtube, stack-exchanges and chatgpt, they access our tool via a shortcut, describe their issue in natural language and get the requisite solution along with visual and auditory cues.&lt;/p&gt;
&lt;p&gt;this could further transition into the opportunity of integrating this with core apps such as the ones aforementioned and provide real time assistants (this could be a business opportunity right here).&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[2 july 2025] [1700 hrs]&lt;/p&gt;
&lt;p&gt;i looked at gemini live and also ran it via the terminal using their api. it is pretty much the barebones version of what i wanna achieve, still there is a lot to be done.&lt;/p&gt;
&lt;p&gt;this is how it works using their api:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;uses pyaudio to capture audio&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;records screen/video frames using cv2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;then uses this to create a session with gemini live and the streaming starts&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;client&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;aio&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;live&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;connect(model, config)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py&#34;&gt;https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;^^ this is the code for the api implementation&lt;/p&gt;
&lt;p&gt;now, the issue is, this wont work with local models, gemma 3n in this case. why? because gemma does not support this streaming feature which is being leveraged above.&lt;/p&gt;
&lt;p&gt;and local ai, irrespective of the hackathons instructions, is important. you wouldnt want to share everything you do on your pc with google servers sitting around the globe in california.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[2 july 2025] [2300 hrs]&lt;/p&gt;
&lt;p&gt;cool so gemma 3n does not accept video inputs. and well, even if it did, it’d have been a nightmare for them to be processed locally, while screen capture is on. doesnt accept mp3 too (and prolly other audio formats also).&lt;/p&gt;
&lt;p&gt;i’ll try sending the final snapshot. dont know how effective that is going to be, but is prolly the only option i have.&lt;/p&gt;
&lt;p&gt;also, the image and the user prompt will have to be pretexted by some context as to what the application is and, and idk what.&lt;/p&gt;
&lt;p&gt;for the prompt, i will take in the user audio - use a speech to text.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[3 july 2025] [1300 hrs]&lt;/p&gt;
&lt;p&gt;the above approach is more or less okay but definitely not the best. ideally, what i would want to achieve is the screen being recorded continuously. everything that has been recorded before the user’s questions goes to an visual llm for understanding, which is then summarised, ie, a summary of everything that has happened since the starting/last prompt till now. the summary should then be passed along with the latest frame and the user’s prompt.&lt;/p&gt;
&lt;p&gt;the common thing between both the two approaches is the latter part, ie, the last frame and the user prompt.&lt;/p&gt;
&lt;p&gt;so for the first iteration, i’ll use the first approach - basic context, last frame and user prompt. for the second iteration - summary of all activities till now, last frame and user prompt.&lt;/p&gt;
&lt;p&gt;let the hacking begin!&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;version-01&#34;&gt;version 0.1&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;[3 july 2025] [1307 hrs]&lt;/p&gt;
&lt;p&gt;so now, i need to think of a pipeline for screen capture at the final frame and user input.&lt;/p&gt;
&lt;p&gt;let me just have text input for starters.&lt;/p&gt;
&lt;p&gt;the flow should look like this:&lt;/p&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;div class=&#34;mermaid&#34;&gt;
        flowchart LR;
        A[shortcut]---&gt;|opens overlay|B[app interface]---&gt;|user entered prompt|D[llm]
        B---&gt;|last frame screenshot|D---&gt;|response|B
    &lt;/div&gt;

&lt;/div&gt;
&lt;p&gt;visual highlighting of the steps involved might be complex. but koina, kar lenge.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[1800 hrs]&lt;/p&gt;
&lt;p&gt;found a way to run multimodal gemma 3n locally, using hugging face transformers.&lt;/p&gt;
&lt;p&gt;here’s the documentation: &lt;a href=&#34;https://ai.google.dev/gemma/docs/core/huggingface_inference&#34;&gt;https://ai.google.dev/gemma/docs/core/huggingface_inference&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;the issue here is, i do not have much experience with hugging face and especially local models.&lt;/p&gt;
&lt;p&gt;what i’ve understood till now is, i have to login via a huggingface token, create a pipeline and then pass the img, text to the pipeline.&lt;/p&gt;
&lt;p&gt;upon doing the above, it gives an error&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;OSError: We couldn&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;t connect to &amp;#39;&lt;/span&gt;https://huggingface.co&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; to load the files, and couldn&amp;#39;&lt;/span&gt;t find them in the cached files.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Check your internet connection or see how to run the library in offline mode at &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://huggingface.co/docs/transformers/installation#offline-mode&amp;#39;&lt;/span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;fixed - there was an issue with the hf token i had initialised. created a new one and got it done.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[4 july 2025] [0015 hrs]&lt;/p&gt;
&lt;p&gt;so i have been able to come up with a very very barebones version (version 0.0000001):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;it uses the &lt;code&gt;pyautogui&lt;/code&gt; library to take a screenshot whenever the script is run,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;pipeline&lt;/code&gt; from hugging face’s &lt;code&gt;transformers&lt;/code&gt; library to create a pipeline of &lt;code&gt;gemma-3n-E2B-it&lt;/code&gt; model,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;and &lt;code&gt;tkinter&lt;/code&gt; for the gui.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tkinter &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; tk
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tkinter &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; scrolledtext, messagebox
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pyautogui
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tempfile
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image, ImageTk
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pipeline
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; huggingface_hub &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; login
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; threading
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;HF_TOKEN &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MODEL_ID &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;google/gemma-3n-E2B-it&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;login(HF_TOKEN)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pipeline(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;image-text-to-text&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;MODEL_ID,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    device&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;capture_screenshot&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; str:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    temp_dir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tempfile&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gettempdir()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    screenshot_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(temp_dir, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gemma_screenshot.png&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    screenshot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pyautogui&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;screenshot()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    screenshot&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(screenshot_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; screenshot_path
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;generate_response&lt;/span&gt;(prompt: str, image_path: str) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; str:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(image_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Image loaded successfully&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; e:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;RuntimeError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Failed to open image: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;e&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    chat_prompt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;: [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;text&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;text&amp;#34;&lt;/span&gt;: prompt},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;: img}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chat prompt created successfully&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pipe(chat_prompt)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Inference completed successfully&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; e:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;RuntimeError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Failed to generate response: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;e&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; output[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;generated_text&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ScreenshotGemmaApp&lt;/span&gt;(tk&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Tk):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;__init__&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        super()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;__init__&lt;/span&gt;()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gemma hack&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;geometry(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;300x900&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;screenshot_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; capture_screenshot()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Show screenshot preview&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;screenshot_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize((&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;))  &lt;span style=&#34;color:#75715e&#34;&gt;# Resize for preview&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tk_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ImageTk&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;PhotoImage(img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;preview_label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tk&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Label(self, image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tk_img)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;preview_label&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pack(pady&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; e:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;⚠️ Failed to show preview:&amp;#34;&lt;/span&gt;, e)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;prompt_box &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tk&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Text(self, height&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, font&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Arial&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;prompt_box&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pack(padx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, pady&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;), fill&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;submit_button &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tk&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Button(self, text&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Submit&amp;#34;&lt;/span&gt;, font&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Arial&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;), command&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;on_submit)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;submit_button&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pack(padx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, pady&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;output_box &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; scrolledtext&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ScrolledText(self, font&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Arial&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;output_box&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pack(padx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, pady&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;), fill&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;both&amp;#34;&lt;/span&gt;, expand&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;on_submit&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        prompt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;prompt_box&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1.0&amp;#34;&lt;/span&gt;, tk&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;END)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; prompt:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            messagebox&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;showwarning(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Missing Prompt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Please enter a prompt.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;submit_button&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;config(state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;disabled&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;output_box&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1.0&amp;#34;&lt;/span&gt;, tk&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;END)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        threading&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Thread(target&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_run_inference_thread, args&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(prompt,), daemon&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;start()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_run_inference_thread&lt;/span&gt;(self, prompt: str):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;starting inference&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; generate_response(prompt, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;screenshot_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;response received&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;output_box&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;insert(tk&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;END, response)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;response:&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(response)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;end of response&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; e:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            messagebox&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;showerror(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error&amp;#34;&lt;/span&gt;, str(e))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;finally&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;submit_button&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;config(state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;normal&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    app &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ScreenshotGemmaApp()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    app&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mainloop()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;on running it, it wasnt giving any response. so i added a couple of lines to help me debug.&lt;/p&gt;
&lt;p&gt;and guess what, the code is absolutely correct, down to the last inch, it is google to blame.&lt;/p&gt;
&lt;p&gt;the model is running slow, like insanely slow. i tried switching over to my gpu by setting &lt;code&gt;device&lt;/code&gt; equal to &lt;code&gt;0&lt;/code&gt;, and boom my computer went for a crash. it used something called an MPS, not CUDA ofc because my dearest gpu wasn’t manufactured in nvidia furnaces. so the error was due to my memory capacity not being enough.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;RuntimeError: MPS backend out of memory &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;MPS allocated: 5.73 GB, other allocations: 384.00 KB, max allowed: 9.07 GB&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Tried to allocate 3.75 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0.0 to disable upper limit &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; memory allocations &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;may cause system failure&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;i went back to cpu then, and after an eternity (8 mins) i have now recd the following response:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;i had also added a cap on the tokens, for now, to make it run faster. and yes, 8 mins was with a token limit of a mere 128.&lt;/p&gt;
&lt;p&gt;i think there would be a fix to this problem. the very reason google came up with gemma 3n was so it could run on edge devices including mobile phones, which, even the best ones, arent even 30% as beefy as my mac (not saying that ts is great).&lt;/p&gt;
&lt;p&gt;google’s edge ai app works on androids and does image inference. i will look into it in the morning and see how they do it. secondly, i’ll revisit this &lt;a href=&#34;https://github.com/OminousIndustries/Gemma3n-TTS&#34;&gt;repo&lt;/a&gt; i used as reference earlier. third, i’ll look into the documentation again, and maybe what other people are doing for this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;i may have to later pivot to this not being truly local (while still using gemma 3n for hackathon purposes) and even further to a better, sota cloud model.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;here are the tasks for morning now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; figure out how to run it faster
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; go through google edge ai&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; go through gemma documentation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; go through this &lt;a href=&#34;https://github.com/OminousIndustries/Gemma3n-TTS&#34;&gt;repo&lt;/a&gt; (talked about it above) and its corresponding yt vid&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; chat gpt in case nothing works&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;tasks for later&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; look into cluely’s (not open source so fake cluely’s) source code&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;using the code given in gemma documentation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the only difference in my current code and the one given in gemma’s documentation is this one line:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch_dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bfloat16
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;i have added this to the pipeline args. lets see how it does now.&lt;/p&gt;
&lt;p&gt;dont think it’s making much of a difference. taking more than 10 secs so not there. will update the final time when i receive the response.&lt;/p&gt;
&lt;p&gt;also, for some reason the response is still cut off at the end.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;using the code in this &lt;a href=&#34;https://github.com/OminousIndustries/Gemma3n-TTS&#34;&gt;repo&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;until now, i have called the model using a pipeline. now, using the above repo as reference, i’ll call it in a different way and see if it helps.&lt;/li&gt;
&lt;li&gt;running into many errors.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;i donot understand what the issue is. i know it can be fixed, i know that for a fact. but how is something i havent been able to figure out yet. i know for sure that it’s hidden in plain sight.&lt;/p&gt;
&lt;p&gt;ok, fukkit. truly locla is hard to achieve, atleast for now. i ll chnage it to online inference.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[7 july 2025][0036 hrs]&lt;/p&gt;
&lt;p&gt;very long time since i last updated this blog or whatever this is. so i’ve been running it on online inference for now, using the gemini-2.0-flash-exp, which i plan on changing to 2.5-flash atleast.&lt;/p&gt;
&lt;p&gt;i also vibecoded a slick looking ui; i tried integrating the concepts of glasmorphism, taking inspiration from apple (&lt;em&gt;good artists copy, great artists steal (jobs))&lt;/em&gt;. surely isnt as goated as apple’s liquid glass but it does it’s job to some extent. here’s how it looks:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image1.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image2.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;there are a few ui changes im still to make:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; allow repositioning&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; answer streaming isnt smooth&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; corners do not have the same curvature&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; a copy button for code in the answer box&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; an icon for enter&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; a settings hamburger menu&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; drastic change: possibly come up with some completely new interface; take inspiration from apple intelligence maybe (oml apple is so goated all my inspirations come from them)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;honestly, while going through all the ui stuff, i was loving it. i have always had a knack for design; kinda ocd-ish sometimes, especially when symmetry is involved. and oh reading about glassmorphism, neumorphism, such an interesting, creative pursuit design is. also read about scandinavian design, just beautiful.&lt;/p&gt;
&lt;p&gt;i resonate deep with steve when he says - &lt;em&gt;it’s technology, married with liberal arts, married with the humanities, that yields us the results that make our hearts sing.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;ok now, on the technology side of things. the backend is shitty. there is absolutely no system instruction, i added some debugging lines that are still being printed. and some other issues too. so do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add system instructions&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; get rid of the debugging statements&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; the screenshot is clicked on each enter, ie, it also has the widget in it; need to change it to be clicked before the widget is shown&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;— i think up till here will complete most part of this app, then i shall put all focus into doing the visual guide part - the most important bit of this.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[7 july 2025][1826 hrs]&lt;/p&gt;
&lt;p&gt;the screenshot issue hasnt been fixed. rn what is happening is that the screenshot is clicked at the press of ‘enter’. which is fine from the user’s perspective because the screenshot contains the latest version of what’s on the screen. but is an issue as the screenshot also contains the ask bar, and incase of a followup, also the answer. which makes it hard to see what is behind it on the actual screen.&lt;/p&gt;
&lt;p&gt;i looked at what other apps are doing:&lt;/p&gt;
&lt;p&gt;horizon - clicks the ss right after the tool shortcut is called, ie, before the chat box appears.&lt;/p&gt;
&lt;p&gt;glass - works a lot better. it takes updates real time, and also stays consistently over all desktops. also, the responses are a lot better. ig that is the difference between a fun project and a startup.&lt;/p&gt;
&lt;p&gt;after trying around a variety of things, i’ve finally settled on making the widget disappear for a while for the screenshot to be captured and then the answer be displayed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;this isnt the most efficient approach. for the final version, the guide, i plan on making a different ui and taking inspiration from &lt;a href=&#34;https://github.com/sohzm/cheating-daddy&#34;&gt;cheatingdaddy&lt;/a&gt; to ensure the above in a more streamlined way.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;now, i’ll just make it ready for deployment. put in on github and focus on the other bits.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[8 july 2025][1700 hrs]&lt;/p&gt;
&lt;p&gt;i have pushed it to github. there are a few issues with it, but i have asked a friend to make the changes required to make it fully deployable while i focus on the main bit — the actual model.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[8 july 2025][1701 hrs]&lt;/p&gt;
&lt;p&gt;i need to firstly analyse how agents work.&lt;/p&gt;
&lt;p&gt;as of now, i have come up with the following pipeline:&lt;/p&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;div class=&#34;mermaid&#34;&gt;
        graph TD;
        screen --&gt; |screenshot or live screencast|omniparser --&gt; |identifies, maps key ui elements|llm
        prompt --&gt; llm --&gt; |action to be performed|visual_cues --&gt;|user| action --&gt; |real time feedback|screen
    &lt;/div&gt;

&lt;/div&gt;
&lt;p&gt;however, what is not clear is how the action and feedback loop works, what exactly the llm outputs when asked the prompt — whether it gives the entire process’ task list (improbable) or gives the first step(s) which is(are) most likely to be correct and once that’s done with, reanalyses the screen and then gives the next one(s).&lt;/p&gt;
&lt;p&gt;what i am thinking would be the best is having a list of broad steps to be outputted first, followed by detailed descriptions for each step one by one, being displayed after the previous one has been executed.&lt;/p&gt;
&lt;p&gt;i think creating an agent is easier than creating a guide as now, people have autonomy to do things, which means, one ill step and the ai gets confused and starts malfunctioning.&lt;/p&gt;
&lt;p&gt;ok let it be. i’ll study more about agents and then see where this goes.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[8 july 1800 hrs]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; check out browser-use&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; check out warm wind&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; check out omni parser&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image3.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;^^warmwind&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image4.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;^^skyvern&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;some yt video:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1GV4VzhfI8l2uEBm2H9hQ2fs12_iFiYlQ?usp=sharing#scrollTo=x1Edd6dsflaa&#34;&gt;https://colab.research.google.com/drive/1GV4VzhfI8l2uEBm2H9hQ2fs12_iFiYlQ?usp=sharing#scrollTo=x1Edd6dsflaa&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=Qnp4PQTE1Ag&#34;&gt;https://www.youtube.com/watch?v=Qnp4PQTE1Ag&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;os atlas:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;also a library like omniparser, but this returns the pixel coords of the gui element.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/OS-Copilot/OS-Atlas&#34;&gt;https://github.com/OS-Copilot/OS-Atlas&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;browser_use:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image5.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;[9 july 2025][1615 hrs]&lt;/p&gt;
&lt;p&gt;i have tried to understand how different computer use agents work and have linked a few repos, screenshots and descriptions above of the ones i found useful.&lt;/p&gt;
&lt;p&gt;the pipeline i had made above, wasnt very off. the area’s where it was off, were actually the areas i was confused in. so i’ll make the requisite changes, and let the hacking begin!&lt;/p&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;div class=&#34;mermaid&#34;&gt;
        graph TD;
        user--&gt;|keyboard shortcut|A[user interface] --&gt; |user prompt|B[planner llm]
        A--&gt;|screenshot*|B
        B--&gt;|task plan|C[task list]--&gt;|single task|D[llm]
        A--&gt;|screenshot*|D--&gt;|detailed action description + button/field indices|E[visual cue]
        D--&gt;|pixel coords of action spot|E[visual cue]--&gt;A
        D--&gt;|update|C
    &lt;/div&gt;

&lt;/div&gt;
&lt;p&gt;+feedback loop to be added&lt;/p&gt;
&lt;p&gt;*screenshots pass through omniparser&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/145e733e-62f7-4fdb-9326-59b17d4e0132.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;^^ easier to understand. i still love pen an pencil over these plastic keycaps.&lt;/p&gt;
&lt;p&gt;validator - success, inaction, failure.&lt;/p&gt;
&lt;p&gt;here’s a written version of what i have thought:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;1.&lt;/span&gt; the user calls the app using a shortcut, types their issue (prompt) on the interface
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;2.&lt;/span&gt; the app takes a screenshot, gives it to omniparser which annotates the image to index and tag interactive ui elements
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;3.&lt;/span&gt; the annotated screenshot and prompt goes to the planning llm (llm #1) which devises a list of tasks to be performed to achieve the goal. each tasks include 1 action at max. (for eg: for export to pdf the tasks are - go to file menu, go to export, choose pdf, save)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;4.&lt;/span&gt; these tasks then go one by one to llm #2 along with a screenshot of the current state of the screen. the decides what action is to be done to complete the task (eg: for &amp;#39;go to export&amp;#39; - click &amp;#39;export&amp;#39; button) and also gives the coordinates of the button.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;5.&lt;/span&gt; this description (click &amp;#39;export&amp;#39; button) and coordinates is used to highlight the respective button and show it to the user.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;6.&lt;/span&gt; the app waits for user action. once done, it moves forward to sending the next task to llm#2 and this continues till all the tasks are done
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ok so for the first version, i’m gonna let cursor do it. i need to understand how these various libraries work and get an idea of how it will all play together. i know the code wont be neat and debugging would be a nightmare for me cuz i haven’t coded it out. but i think it’ll do well for starters, to make me understand the technicalities of it and then later i can code it out with my bare hands.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;vib(code)ing&lt;/p&gt;
&lt;p&gt;this is the task list cursor came up with after i gave my pipeline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;technical task list&lt;/p&gt;
&lt;h3 id=&#34;1electron-overlayapp&#34;&gt;&lt;strong&gt;1. Electron Overlay App&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Set up Electron app with cross-platform build (Mac/Windows).&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Implement global keyboard shortcut listener.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Create overlay UI for prompt input and step-by-step guidance.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Implement overlay rendering for highlights and descriptions (transparent, click-through except for UI).&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Ensure overlay can be shown/hidden and does not interfere with normal app usage.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2screenshotcapture&#34;&gt;&lt;strong&gt;2. Screenshot Capture&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Implement cross-platform screenshot capture (Node.js native modules or external tools).&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Ensure screenshot is of the correct display (multi-monitor support).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3omniparser-integration&#34;&gt;&lt;strong&gt;3. Omniparser Integration&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Integrate with Omniparser (Python or API) to send screenshots and receive annotated UI elements.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Parse and store Omniparser output for downstream use.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4llm1-task-planning&#34;&gt;&lt;strong&gt;4. LLM #1: Task Planning&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Integrate with Gemini API (or other LLM) for planning.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Send prompt + annotated screenshot, receive ordered list of atomic tasks.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Parse and store task list.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5llm2-action-elementlocalization&#34;&gt;&lt;strong&gt;5. LLM #2: Action &amp;amp; Element Localization&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; For each task, send current screenshot + task to LLM #2.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Receive action description and UI element coordinates.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Parse and store action/coordinates.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;6visual-guidance-user-interaction&#34;&gt;&lt;strong&gt;6. Visual Guidance &amp;amp; User Interaction&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Render highlight around UI element at given coordinates.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Display action description in overlay.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Track user actions (mouse clicks, keyboard events) to detect when the correct action is performed.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Handle edge cases (user clicks wrong element, window focus changes, etc.).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;7taskprogression-logic&#34;&gt;&lt;strong&gt;7. Task Progression Logic&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Manage task queue and progression.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; On correct user action, advance to next task.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; On completion, show success message and dismiss overlay.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;8cross-processcommunication&#34;&gt;&lt;strong&gt;8. Cross-Process Communication&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Set up IPC (inter-process communication) between Electron (Node.js) and Python (Omniparser, LLM calls).&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Ensure robust error handling and fallback for communication failures.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;9cross-platformpackaging-deployment&#34;&gt;&lt;strong&gt;9. Cross-Platform Packaging &amp;amp; Deployment&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Package the app for Mac and Windows (Electron Builder, etc.).&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Bundle Python dependencies and ensure they run on both platforms (PyInstaller, virtualenv, etc.).&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Handle permissions (screen recording, accessibility APIs) for both OSes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;10configuration-extensibility&#34;&gt;&lt;strong&gt;10. Configuration &amp;amp; Extensibility&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Configurable keyboard shortcut.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Modular architecture for swapping out LLMs or UI parsers.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Logging and error reporting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;^^ looks pretty robust, i’ll let it start coding.&lt;/p&gt;
&lt;p&gt;ok this is f-ing my brains inside out. it feels like a blackbox, i dont understand a thing and the only thing i can do is plead like a begger to have cursor solve the errors. yet, despite the errors im gonna let it continue. so that i atleast know how its trying to integrate all of various things and then i’ll start myself.&lt;/p&gt;
&lt;p&gt;ok so the final thing isnt working due to a hell lot of errors, but here is a technical report i asked cursor to make, so my understanding about what it did in the mvp is bettered.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://1rvinn.github.io/img/build/bubble/report_-_bubble_v2.pdf&#34;&gt;report - bubble_v2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;[11 july 2025][2222 hrs]&lt;/p&gt;
&lt;p&gt;so, last 1.5 ish days, i have been learning js. and i think i have understood it’s basics well. on second thought, i shouldve have spent this much time, the real output has pretty much been 0, as i thought of it to be pretty similar apart from the basic syntax level differences. also, at this point i think, if you have a decent hold over one programming language, it shouldnt be hard to understand others and a more effecient way to go about it would be just to build a project and learn on the fly.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; im just gonna go through the basics of electron really quick, and then get started with coding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ok, some basics covered, rest will be dealt with by referring to the documentation.&lt;/p&gt;
&lt;p&gt;here’s a great github repo having a ton of boilerplates, tutorials and demo code:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sindresorhus/awesome-electron&#34;&gt;https://github.com/sindresorhus/awesome-electron&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;this could be referred to for screenshots/sharing:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/hokein/electron-screen-recorder&#34;&gt;https://github.com/hokein/electron-screen-recorder&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; next, i need to cover asyncio&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; and backend hosting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;also then, i need to look into how cheating daddy works, majorly focusing on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; how are screenshots captured? is it a live stream (using gemini live) or screenshots when the prompt is asked&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; how do they make it resilient to screensharing&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;[13 july 2025][1800 hrs]&lt;/p&gt;
&lt;p&gt;time to get my fingertips dirty, let the hacking begin!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;screenshot parsing&lt;/p&gt;
&lt;p&gt;i tried object detection via gemini as mentioned in their documentation but it didn’t work well. so i’ll just try setting up omniparser.&lt;/p&gt;
&lt;p&gt;for now, i’ll just run it off hf transformers, later i might set up an hf inference end point and call it via cloud.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;[17 july 2025][1118 hrs]&lt;/p&gt;
&lt;p&gt;i have pretty much wasted the last 3-4 days. need to lock in now.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;screenshot parsing&lt;/p&gt;
&lt;p&gt;i tried integrating omniparser but no luck. need to do the following, if it still doesn’t go through after this, then we skip to a different model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ask claude for help&lt;/p&gt;
&lt;p&gt;^gives the same old shit&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;i’ve given omniparser a lot of time, but no avail. i even tried to run the entire code by cloning the repo but even that refused to work. so now, im looking to switch over to something else.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; look into browser-use’s screenshot parsing mech&lt;/p&gt;
&lt;p&gt;^^they use playwright which only works for browsers so cant be implemented&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ok nvm, omniparser worked. i was running into a variety of different module errors and despite multiple tries, i wasnt able to figure it out, but at the end, i prevailed.&lt;/p&gt;
&lt;p&gt;for now, it runs on gradio and o boy gradio is amazing. i can literally use the live link to run omniparser through any device while using my laptop for processing. also, i can use it as an api pretty easily.&lt;/p&gt;
&lt;p&gt;however, there are a few issues with this currently:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;the process takes a lot of time, 30 to 300 secs for images with less and more icons respectively. it isnt the detection algorithm that takes this much time but the caption model instead that takes up most of the time. the former takes less than a second.&lt;/p&gt;
&lt;p&gt;the model being used to generate captions is &lt;code&gt;florence2&lt;/code&gt; or &lt;code&gt;blip2&lt;/code&gt; which are vision language models, so ofc are bound to take time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;i tried running it using the gradio api hosted on hf spaces by microsoft itself, which is returning the result quick, however i am not able to access the image being generated by it. this is what it returns: &lt;code&gt;&#39;/private/var/folders/60/w....wgn/T/gradio/2...adfa/image.webp&#39;&lt;/code&gt; which means it’s just returning the local path on the server and not the image.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;it is imperative to solve these things, but later. for now, i’ll code rest of the app and then finally find a fix to the above, prolly by hosting it somewhere - &lt;a href=&#34;http://fly.io&#34;&gt;fly.io&lt;/a&gt; or hf spaces.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;[20 jul 2025][1450 hrs]&lt;/p&gt;
&lt;p&gt;i’ll start building the planner llm. just a regular gemini 2.5 flash call along with the screenshot being sent.&lt;/p&gt;
&lt;p&gt;i tested it out by sending a screenshot of the chatgpt interface without any annotations and gave this prompt &lt;code&gt;how do i start a new chat and upload a photo to it?&lt;/code&gt; . here’s the response i got, pretty happy with it for now.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image6.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;next, i ll work on making the second llm - the action selector.&lt;/p&gt;
&lt;p&gt;the prompt was &lt;code&gt;Click the &#39;+&#39; button next to the &#39;Ask anything&#39; text input field.&lt;/code&gt; and it correctly gave me the right icon to be selected.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image7.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;next steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create a screenshot capturer&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; combine planner and action selector&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;one thing i’ve realised is that the captions being generated aren’t very precise and in fact making the action_selector select the wrong element.
secondly the caption generation model is the reason for such slow responses. i am thinking to let it go, and my hypothesis is that the action_selector will only get more proficient in doing so. however, i do need to test this hypothesis.&lt;/p&gt;
&lt;p&gt;ok, now how do i go about removing the captions? in the model i run locally, it is pretty easy since i can play with the code. however, since the model is being called using an api endpoint here, i can’t possibly change the code and neither do i see an option to disable captions being generated.
so maybe i’ll just deploy it locally and use it as an api (possible with gradio)&lt;/p&gt;
&lt;p&gt;i have been able to remove the captioning and it takes way lesser than what it used to earlier, with only the annotation happening right now. however, the icon selection is still a little haywire.&lt;/p&gt;
&lt;p&gt;before fixing that, i think a couple of other things need to be done:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; make the planner llm only give out broad descriptions not precise ones (exact button presses not needed)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; make the action selector llm decide the specifics of the atomic tasks to be performed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;the above plan didnt work out, since one task of the planner’s plan can possibly contain multiple actions to be performed. so, i’ve added an atomic generator model that takes in tasks, one by one from the plan, and divides them into actions to be performed based on the screenshot input&lt;/p&gt;
&lt;p&gt;so they new pipeline looks as follows&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/Mermaid_Chart_-_Create_complex_visual_diagrams_with_text._A_smarter_way_of_creating_diagrams.-2025-07-25-093021.svg&#34; alt=&#34;Mermaid Chart - Create complex, visual diagrams with text. A smarter way of creating diagrams.-2025-07-25-093021.svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;this is working way better.&lt;/p&gt;
&lt;p&gt;next, i need to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; figure out a way to get the image returned as well&lt;/p&gt;
&lt;p&gt;^^ for this, i changed the api’s output from returning a pil image to the base64 string of that image&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; integrate the cue and automatic screenshots&lt;/p&gt;
&lt;p&gt;^^ i have used the keystroke &lt;code&gt;ctrl+shift+0&lt;/code&gt; to click a screenshot before every step be converted to atomic steps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create 2 versions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; one with the planner&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; one without the planner - only the atomic generator deciding what is to be done on the basis of current state, end goal and previous actions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;i think the second version is bound to work better. currently the planner is working on foresight. it is predicting what must be done without having access to the current state. it just takes the initial state and decides the plan. in the case without the planner, the current step is only decided on the basis of the current state, the user intent and current history.
barring the foresight problem, there is also the issue of inefficiency. currently we are employing two models just to decide on the atomic tasks that need to be completed as opposed to one in case the planner is dropped.&lt;/p&gt;
&lt;p&gt;hence, i feel its imperative to make the second version the main one.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;the second version is up and running, and it works pretty damn well. way better than the first/planner one.&lt;/p&gt;
&lt;p&gt;there are a few improvements yet to be made:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; integrate a validator that verifies whether the previous action has been completed successfully or the user messed up&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; need to be very specific on which button to click in case of multiple buttons with the same label&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;i’ll do the above later. for now, i’ll focus on getting the frontend ready.&lt;/p&gt;
&lt;p&gt;the frontend will be made using electron js, exact designs for which i havent figured yet.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.electronjs.org/docs/latest&#34;&gt;https://www.electronjs.org/docs/latest&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;^^ keeping the documentation link here for easy access.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;so i’ve been able to come up with a raw frontend. not the most aesthetically pleasing app, however, it suffices for the hackathon submission atleast.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image8.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;however, there still are a few issues with this, which i need to fix:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; instead of image , upload base 64 upload to omniparser&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; colors and aesthetics improve&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; make sure bounding boxes are complete /neat and adjust the vertical alignment in the bounding box&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; bounding box moves /shifts if user scrolls in between. can we fix that -
A) make the process very fast
B) control the webpage (not relevant now)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; integrate a validator that verifies whether the previous action has been completed successfully or the user messed up&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; need to be very specific on which button to click in case of multiple buttons with the same label&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; giving the user flexibility to choose/fill, in case of forms.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ctrl+shift+0 for next step ctrl+shift+1 for retry&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; improve latency
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; change gemini calls also to base64&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove the drop shadow at the center of the screen&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; rename ‘processing’ messages&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; disappearance on click&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; clickthough not errorless&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; rephrase task descriptions&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; completed message&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; make it windows compatible&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; doesnt enter typing mode on first open, fix that&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove the settings button&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; press crtl+shift+0 for next step&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; fix history handling&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; thinking box visible in screenshots&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; remove ctrl+shift+f ctrl /&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; make click global&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; description being cutoff&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; add glow to border&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; fix ‘input focused’ wording and positioning&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; task completed positioning - vertical and horizontal both&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; scrolling&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; instead of the bounding box, make the description box emerge from the place/button that needs to be interacted with, like a comment box.&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ctrl+shift+0 not working properly&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; vertical offset in windows&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; entire pipeline should restart when task gets completed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;pretty much ready for the submission.&lt;/p&gt;
&lt;p&gt;here is the final pipeline being used:&lt;/p&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;div class=&#34;mermaid&#34;&gt;
        flowchart TD;
        A[user prompt]--&gt;D[atomic task generator]
        C[current screen state]--&gt;D
        B[task history]--&gt;D
        D--&gt;|next atomic task to be executed|E[element selector]
        C--&gt;F[omniparser]--&gt;|annotated image|E--&gt;|element coordinates|G[gui/frontend]
        D--&gt;|task description|G--&gt;|execution success/failure|B
    &lt;/div&gt;

&lt;/div&gt;
&lt;p&gt;here are a few snapshots:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image9.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image10.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image11.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image12.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image13.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/bubble/image14.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;pushing to github:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; create 2 repos:
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; bubble_v2&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; bubble_omni (for the omniparser backend)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; check requirements.txt&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; readme
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; banner&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; short description&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; video demo&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; pipeline&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; how to operate
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ctrl/cmd+shift+g to call the app&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ctrl/cmd+shift+0 for the next step&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ctrl/cmd+shift+1 to retry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; how to run locally - instructions to run locally
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; venv, install reqs at backend/requirements.txt&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; env, add hf token, gemini api key&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; run the omni backend, get endpoint url, update in backend/omni_api
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; venv, install reqs&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; login using hf token - hugging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; npm install, npm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;done&#34;&gt;done!&lt;/h3&gt;
</content>
    </item>
    
    <item>
      <title>project proposal - federated learning</title>
      <link>https://1rvinn.github.io/crude/project-proposal---federated-learning/</link>
      <pubDate>Tue, 29 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://1rvinn.github.io/crude/project-proposal---federated-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;this project aims at exploring a novel framework for training neural networks - federated learning.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;brief-about-federated-learning&#34;&gt;brief about federated learning:&lt;/h4&gt;
&lt;p&gt;traditionally, local availability of training data has been an important pre-requisite for training ml models. however, this methodology has a variety of road blocks associated with it. the prime ones being lack of democratisation in model creation and lower development in fields with sensitive data like finance and healthcare. federated learning aims to solve these challenges by introducing an alternate way of training models - instead of data travelling to the model and the model getting trained, the model travels to data sources (silos, personal devices), gets trained and publishes the updates back to the central model.
this enhances the privacy of ai based systems since the data has to never leave the device. furthermore, this means that people can have full control of their data and yet contirbute tot he development of ai, ie, now proprietary and sensitive data can also be used for training (both pre and post) purposes. this solves the problem of only big data companies being able to train big models, thus leading to more democratization in the field. secondly, it also promotes ai developments in the sensitive data fields like finance and healthcare, which traditionally have been underdeveloped due to privacy concerns.&lt;/p&gt;</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;this project aims at exploring a novel framework for training neural networks - federated learning.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;brief-about-federated-learning&#34;&gt;brief about federated learning:&lt;/h4&gt;
&lt;p&gt;traditionally, local availability of training data has been an important pre-requisite for training ml models. however, this methodology has a variety of road blocks associated with it. the prime ones being lack of democratisation in model creation and lower development in fields with sensitive data like finance and healthcare. federated learning aims to solve these challenges by introducing an alternate way of training models - instead of data travelling to the model and the model getting trained, the model travels to data sources (silos, personal devices), gets trained and publishes the updates back to the central model.
this enhances the privacy of ai based systems since the data has to never leave the device. furthermore, this means that people can have full control of their data and yet contirbute tot he development of ai, ie, now proprietary and sensitive data can also be used for training (both pre and post) purposes. this solves the problem of only big data companies being able to train big models, thus leading to more democratization in the field. secondly, it also promotes ai developments in the sensitive data fields like finance and healthcare, which traditionally have been underdeveloped due to privacy concerns.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.dailydoseofds.com/content/images/2023/11/federated-gif.gif&#34; alt=&#34;federated learning&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a comic by google on the topic: &lt;a href=&#34;https://federated.withgoogle.com/&#34;&gt;https://federated.withgoogle.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;an interactive visualisation covering the basics: &lt;a href=&#34;https://pair.withgoogle.com/explorables/federated-learning/&#34;&gt;https://pair.withgoogle.com/explorables/federated-learning/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;an in depth overview: &lt;a href=&#34;https://queue.acm.org/detail.cfm?id=3501293&#34;&gt;https://queue.acm.org/detail.cfm?id=3501293&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2107.10976&#34;&gt;https://arxiv.org/pdf/2107.10976&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;a small writeup by me on the importance of FL: &lt;a href=&#34;https://1rvinn.github.io/eureka/fed/&#34;&gt;https://1rvinn.github.io/eureka/fed/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;this project will be an amalgamation of ai and security. focusing in depth on the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;how models are trained [A]&lt;/li&gt;
&lt;li&gt;working of neural nets [A]&lt;/li&gt;
&lt;li&gt;comparision between traditionally trained nets vs FL trained ones&lt;/li&gt;
&lt;li&gt;networking between devices and a central server [S]&lt;/li&gt;
&lt;li&gt;privacy preserving methodologies and the mathematics involved [S]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A - ai focused topics, S - security focused topics&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;milestones-to-be-achieved&#34;&gt;milestones to be achieved:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;building the basis (a good part of this can be covered in the app phase) [3 weeks]
&lt;ul&gt;
&lt;li&gt;gain an idea about federated learning (only a high level overview)
&lt;ul&gt;
&lt;li&gt;key steps involved:
&lt;ul&gt;
&lt;li&gt;client selection&lt;/li&gt;
&lt;li&gt;model distribution&lt;/li&gt;
&lt;li&gt;update upload&lt;/li&gt;
&lt;li&gt;aggregation, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/blog-series/privacy-preserving&#34;&gt;understanding privacy considerations&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;model update attacks&lt;/li&gt;
&lt;li&gt;trained model attacks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;current applications in the industry, future prospects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;start off by implementing it on ml models, using already existing frameworks like &lt;a href=&#34;https://flower.ai/&#34;&gt;flower&lt;/a&gt;/&lt;a href=&#34;https://developer.nvidia.com/flare&#34;&gt;nvidia flare&lt;/a&gt; [2 weeks]
&lt;ul&gt;
&lt;li&gt;understanding the library&lt;/li&gt;
&lt;li&gt;implement on a basic ml model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;delving deeper
&lt;ul&gt;
&lt;li&gt;understanding fl in greater detail [3 months]
&lt;ul&gt;
&lt;li&gt;comprehensively understand the maths behind it&lt;/li&gt;
&lt;li&gt;exploring different training methods like FedAvg, FedSGD etc.&lt;/li&gt;
&lt;li&gt;security considerations
&lt;ul&gt;
&lt;li&gt;secure aggregation&lt;/li&gt;
&lt;li&gt;differential privacy&lt;/li&gt;
&lt;li&gt;homomorphic encryption&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;modeling federated learning from scratch
&lt;ul&gt;
&lt;li&gt;coding the process from scratch, including all the above stated steps (&amp;lsquo;key steps involved&amp;rsquo;)&lt;/li&gt;
&lt;li&gt;implementing &amp;gt;=1 privacy preserving strategies (secure agg, diff priv., homo enc)&lt;/li&gt;
&lt;li&gt;facilitating networking between edge devices and a central server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;implementation:
&lt;ul&gt;
&lt;li&gt;implement on the following networks (subject to change on the basis of our findings above): [3 months]
&lt;ul&gt;
&lt;li&gt;CNNs&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.09943&#34;&gt;SLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;implement using existing libraries first [to be displayed in RC] and later from scratch.&lt;/li&gt;
&lt;li&gt;compare their accuracies with that of normally trained neural nets [to be displayed in RC]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;look into fine-tuning llms for particular tasks using FL
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.05976&#34;&gt;FLoRA&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2102.00875&#34;&gt;https://arxiv.org/abs/2102.00875&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;this could be one of the first steps towards making specialised chatbots (possibly even agents) trained on personal private data (great financial use case).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;further improvements
&lt;ul&gt;
&lt;li&gt;coming up with solutions to the bottlenecks identified&lt;/li&gt;
&lt;li&gt;a few unique approaches include
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alphaxiv.org/abs/2209.06359v1&#34;&gt;federated pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.03448&#34;&gt;federated reconstruction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;federated dropout&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;future prospects
&lt;ul&gt;
&lt;li&gt;publishing our own library for federated learning - simplifying tasks like secure aggregation, differential privacy etc.&lt;/li&gt;
&lt;li&gt;an api based federated learning model which people can opt into for training
&lt;ul&gt;
&lt;li&gt;eg:  small financial institutions can opt-in to train anti-fraud or loan default prediction models collaboratively.&lt;/li&gt;
&lt;li&gt;hospitals signing up to develop privacy preserving diagnostic models. &lt;a href=&#34;https://aibusiness.com/verticals/intel-and-upenn-to-use-federated-ai-for-privacy-preserving-brain-tumor-research#:~:text=Instead%20of%20moving%20data%20to,into%20a%20single%2C%20larger%20model&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;creating an agent specialized for a particular task - financial analysis, tax filing agents.&lt;/li&gt;
&lt;li&gt;^ requires collaborations with financial institutions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>mindpalace</title>
      <link>https://1rvinn.github.io/build/mindpalace/</link>
      <pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://1rvinn.github.io/build/mindpalace/</guid>
      <description>&lt;hr&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;h3 style=&#34;color: #23affd;&#34;&gt; // the final thing &lt;/h3&gt;
&lt;p&gt;find it hard to understand long github repos and pdfs?&lt;/p&gt;
&lt;p&gt;struggle no more, just enter your mindpalace.&lt;/p&gt;
&lt;p&gt;mindpalace helps you understand the repo/pdf visually by providing concise explanations along with mindmaps. and the ask ai feature allows you to ask anything about it. hop into your mindpalace - understand visually, understand better&lt;/p&gt;
&lt;p&gt;try it out here: &lt;a href=&#34;https://mindpalace.streamlit.app&#34;&gt;https://mindpalace.streamlit.app&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/SyELL3N9h5c?si=zy4PU5eZPHyrX7sg&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;br&gt;
    &lt;h3 style=&#34;color: #23affd;&#34;&gt; // building it out - the journey&lt;/h3&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;the idea:&lt;/strong&gt;
the user should be able to input pdf(s) which get converted into mindmaps for easy learning. the same should be done for github repos. in addition, there should be an llm to ask questions. &lt;br&gt;
useful for neurodiverse people; facilitates better learning for them&lt;/p&gt;</description>
      <content>&lt;hr&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;h3 style=&#34;color: #23affd;&#34;&gt; // the final thing &lt;/h3&gt;
&lt;p&gt;find it hard to understand long github repos and pdfs?&lt;/p&gt;
&lt;p&gt;struggle no more, just enter your mindpalace.&lt;/p&gt;
&lt;p&gt;mindpalace helps you understand the repo/pdf visually by providing concise explanations along with mindmaps. and the ask ai feature allows you to ask anything about it. hop into your mindpalace - understand visually, understand better&lt;/p&gt;
&lt;p&gt;try it out here: &lt;a href=&#34;https://mindpalace.streamlit.app&#34;&gt;https://mindpalace.streamlit.app&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/SyELL3N9h5c?si=zy4PU5eZPHyrX7sg&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;br&gt;
    &lt;h3 style=&#34;color: #23affd;&#34;&gt; // building it out - the journey&lt;/h3&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;the idea:&lt;/strong&gt;
the user should be able to input pdf(s) which get converted into mindmaps for easy learning. the same should be done for github repos. in addition, there should be an llm to ask questions. &lt;br&gt;
useful for neurodiverse people; facilitates better learning for them&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;inspirations:&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;  1. docagram &lt;/summary&gt;
&lt;p&gt;creates flow-chart summaries from web pages&lt;/p&gt;
&lt;p&gt;flow:
&lt;img src=&#34;https://1rvinn.github.io/img/build/mindpalace/image.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;takes in data from web page sections&lt;/li&gt;
&lt;li&gt;creates a summary of each of those sections&lt;/li&gt;
&lt;li&gt;generates relationships between parts of the section&lt;/li&gt;
&lt;li&gt;has 2 versions -
&lt;ul&gt;
&lt;li&gt;w/o gemini: uses summarizer api and prompt api for the 2 tasks respectively&lt;/li&gt;
&lt;li&gt;w/ gemini&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;content:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/mindpalace/image%201.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;summary:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/mindpalace/image%202.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;summary v2 (used to generate the graph)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/mindpalace/image%203.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;graph/mermaid diagram:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/mindpalace/image%204.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;a shaded box displays the relationship, the unfilled boxes are entities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;gemini result&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/mindpalace/image%205.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;also have the ability to display all graphs together, can choose specific entitites to show as well if someone wants to learn topic wise&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt; 2. better learning for neurodiverse ppl&lt;/summary&gt;
medium article - https://medium.com/@elle.neal_71064/mind-mapping-with-ai-an-accessible-approach-for-neurodiverse-learners-1a74767359ff
&lt;/details&gt;
&lt;p&gt;can build upon these&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;19/02/25 initial plan:&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
    &lt;div class=&#34;mermaid&#34;&gt;

    flowchart TD
        A[document]---&gt;|segmentor|B[segments of data]---&gt;|summarizer|D[summarized segments of data]---&gt;|relationship determiner|E[mermaid diagram]
        A---&gt;C[rag]---&gt;F[llm]
        D---&gt;E
    &lt;/div&gt;

&lt;/div&gt;
&lt;p&gt;// want to create it on streamlit since minimal ui, simpler coding&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;update 1 (19/2/25):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;i feel like i should pivot this towards github. a tool that facilitates understanding of complex repos through mindmap generation and llm integration&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for github repo, i need to parse the contents into an llm acceptable format. there are tools doing that already. i am not sure whether to use them/their apis or develop it completely on my own.
&lt;ul&gt;
&lt;li&gt;one of the ideas i came across recently was that of &lt;a href=&#34;https://www.entelligence.ai/explore&#34;&gt;entelligence.ai&lt;/a&gt; - they take in repos, integrate an llm, provide documentation and flow diagram.
&lt;ul&gt;
&lt;li&gt;the llm responses are really good - looks finetuned (??)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;maybe have both options - github and documents&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multimodal support is crucial - it should be able to take in documents, pictures,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;reading--trying-out-stuff&#34;&gt;reading &amp;amp; trying out stuff&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;(1) should i use beautiful soup to scrap github repo data? prolly not gonna work but never used it so lemme try it out once.&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;trying out beautiful soup &lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;(referring to https://medium.com/@elle.neal_71064/mind-mapping-with-ai-an-accessible-approach-for-neurodiverse-learners-1a74767359ff)

python code:

```python
import requests
from bs4 import BeautifulSoup

def scrape_text(url):
    # Send a GET request to the URL
    response = requests.get(url)

    # If the GET request is successful, the status code will be 200
    if response.status_code == 200:
        # Get the content of the response
        page_content = response.content

        # Create a BeautifulSoup object and specify the parser
        soup = BeautifulSoup(page_content, &amp;quot;html.parser&amp;quot;)

        # Get the text of the soup object
        text = soup.get_text()

        # Return the text
        return text
    else:
        return &amp;quot;Failed to scrape the website&amp;quot;

url=input(&amp;quot;enter url:&amp;quot;)
text=scrape_text(url)
print(text)
```

returns text in the website.
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;didn&amp;rsquo;t try it on github repos, can make it work somehow, will be hard tho. will look into it later.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;(2) how did this guy make the functionality?&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;reading this medium article&lt;/summary&gt;
https://medium.com/@elle.neal_71064/mind-mapping-with-ai-an-accessible-approach-for-neurodiverse-learners-1a74767359ff
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;they’ve given the entire &lt;em&gt;input_text&lt;/em&gt; received after parsing to the model and asked to generate mermaid maps&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;i want mine to be better - it should generate summaries and mermaids for each section, determine the relation among the sections and generate a cohesive mermaid also&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;(3) checking how good chat gpt is with giving mermaid diags&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt; this section&#39;s gonna be long, tried out a variety of inputs and prompts&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;asked chatgpt to analyse my repo - via the repo link - and asked to give a mermaid code explaining it
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;output:&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
      &lt;div class=&#34;mermaid&#34;&gt;
            graph TD
                A[Start] --&gt; B[Load Configuration]
                B --&gt; C[Read Input Data: Orders &amp; Locations]
                C --&gt; D[Fetch Route Data from APIs]
                D --&gt; E[Optimize Routes using Held-Karp Algorithm]
                E --&gt; F[Calculate Emissions Based on Route]
                F --&gt; G[Generate Optimized Route and Emissions Report]
                G --&gt; H[Display Output to User]
                H --&gt; I[End]
                
                %% Dependencies
                D --&gt;|API Calls| J{TomTom API / HERE API}
                J --&gt; D
            &lt;/div&gt;

  &lt;/div&gt;
  the output aint bad
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;then i used &lt;a href=&#34;https://gitingest.com/&#34;&gt;gitingest&lt;/a&gt; to get llm ready parseed version of my repo. gave it as context to gpt4o and asked it to give a mermaid code.&lt;/p&gt;
&lt;p&gt;prompt: [llm ready parsed info (token length - 4.7k)] given the above description of a repository, write code for a mermaid diagram explaining the same&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;output:&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
      &lt;div class=&#34;mermaid&#34;&gt;
                graph TD;
                    A[Repository: 1rvinn-fedexpedite] --&gt;|Contains| B[README.md]
                    A --&gt;|Contains| C[app.py]
                    A --&gt;|Contains| D[requirements.txt]
                    A --&gt;|Contains| E[images/]
                    A --&gt;|Contains| F[.streamlit/]
                    F --&gt; G[config.toml]
                    
                    subgraph &#34;Core Files&#34;
                        B --&gt;|Provides| B1[Project Overview &amp; Setup Instructions]
                        C --&gt;|Main Application| C1[Route Optimiser &amp; Emission Calculator]
                        D --&gt;|Dependencies| D1[Streamlit, Folium, Requests, Streamlit Folium]
                        E --&gt;|Contains| E1[Visual Assets - Screenshots, Diagrams]
                        G --&gt;|Defines| G1[Streamlit Theme &amp; UI Customisation]
                    end
                
                    subgraph &#34;Application Workflow&#34;
                        C1 --&gt;|Receives API Keys| H[TomTom API, HERE API]
                        C1 --&gt;|Takes Inputs| I[Route Preferences, Vehicle Type, Fuel Type]
                        I --&gt;|Uses| J[Held-Karp Algorithm for Optimisation]
                        J --&gt;|Outputs| K[Optimal Route, Travel Time, Emission Data]
                        K --&gt;|Visualised in| L[Google Maps, Folium Map]
                    end
                
                &lt;/div&gt;

  &lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;maybe ask it to give 2 separate diagrams for directory structure and code logic.&lt;/p&gt;
&lt;p&gt;prompt: give 2 separate codes - one explaining the directory structure and the other explaining the code, logic and flow&lt;/p&gt;
&lt;p&gt;output:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;og output: has errors&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
      &lt;div class=&#34;mermaid&#34;&gt;
                        &lt;div class=&#34;mermaid&#34;&gt;
                        graph TD;
                            %% Directory Structure
                            subgraph &#34;Repository Structure&#34;
                                A[Repository: 1rvinn-fedexpedite] --&gt;|Contains| B[README.md]
                                A --&gt;|Contains| C[app.py]
                                A --&gt;|Contains| D[requirements.txt]
                                A --&gt;|Contains| E[images/]
                                A --&gt;|Contains| F[.streamlit/]
                                F --&gt; G[config.toml]
                            end
                        
                            subgraph &#34;Core Files&#34;
                                B --&gt;|Provides| B1[Project Overview &amp; Setup Instructions]
                                C --&gt;|Main Application| C1[Route Optimiser &amp; Emission Calculator]
                                D --&gt;|Dependencies| D1[Streamlit, Folium, Requests, Streamlit Folium]
                                E --&gt;|Contains| E1[Visual Assets (Screenshots, Diagrams)]
                                G --&gt;|Defines| G1[Streamlit Theme &amp; UI Customisation]
                            end
                        &lt;/div&gt;

                
                        &lt;div class=&#34;mermaid&#34;&gt;
                        graph TD;
                            %% Application Logic and Flow
                            A[User Input] --&gt;|Enter Route Details| B[Start Location &amp; Stops]
                            B --&gt;|Fetch Coordinates| C[HERE API]
                            B --&gt;|Select Vehicle Type| D[Emission Factors Calculation]
                            
                            subgraph &#34;Route Calculation&#34;
                                C --&gt;|Compute Routes| E[TomTom API]
                                E --&gt;|Applies| F[Held-Karp Algorithm]
                                F --&gt;|Outputs| G[Optimised Route &amp; Travel Time]
                            end
                            
                            subgraph &#34;Emissions Calculation&#34;
                                D --&gt;|Compute Emissions| H[Vehicle Type, Engine Size, Fuel Type]
                                H --&gt;|Uses Data| I[India GHG Emission Factors]
                                I --&gt;|Outputs| J[Total Emissions (kg CO₂)]
                            end
                            
                            subgraph &#34;Visualisation &amp; Results&#34;
                                G --&gt;|Displays| K[Google Maps &amp; Folium]
                                J --&gt;|Shows| L[Emissions Output]
                                K &amp; L --&gt;|Final Output| M[User Dashboard]
                            end
                        &lt;/div&gt;

                    &lt;/div&gt;

  &lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;output divided into 2&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
                
                graph TD;
                    %% Directory Structure
                    subgraph &#34;Repository Structure&#34;
                        A[Repository: 1rvinn-fedexpedite] --&gt;|Contains| B[README.md]
                        A --&gt;|Contains| C[app.py]
                        A --&gt;|Contains| D[requirements.txt]
                        A --&gt;|Contains| E[images/]
                        A --&gt;|Contains| F[.streamlit/]
                        F --&gt; G[config.toml]
                    end
                
                    subgraph &#34;Core Files&#34;
                        B --&gt;|Provides| B1[Project Overview &amp; Setup Instructions]
                        C --&gt;|Main Application| C1[Route Optimiser &amp; Emission Calculator]
                        D --&gt;|Dependencies| D1[Streamlit, Folium, Requests, Streamlit Folium]
                        E --&gt;|Contains| E1[Visual Assets - Screenshots, Diagrams]
                        G --&gt;|Defines| G1[Streamlit Theme &amp; UI Customisation]
                    end
                &lt;/div&gt;

  &lt;/div&gt;
&lt;p&gt;^ this is good.&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
                graph TD;
                    %% Application Logic and Flow
                    A[User Input] --&gt;|Enter Route Details| B[Start Location &amp; Stops]
                    B --&gt;|Fetch Coordinates| C[HERE API]
                    B --&gt;|Select Vehicle Type| D[Emission Factors Calculation]
                    
                    subgraph &#34;Route Calculation&#34;
                        C --&gt;|Compute Routes| E[TomTom API]
                        E --&gt;|Applies| F[Held-Karp Algorithm]
                        F --&gt;|Outputs| G[Optimised Route &amp; Travel Time]
                    end
                    
                    subgraph &#34;Emissions Calculation&#34;
                        D --&gt;|Compute Emissions| H[Vehicle Type, Engine Size, Fuel Type]
                        H --&gt;|Uses Data| I[India GHG Emission Factors]
                        I --&gt;|Outputs| J[Total Emissions - kg CO₂]
                    end
                    
                    subgraph &#34;Visualisation &amp; Results&#34;
                        G --&gt;|Displays| K[Google Maps &amp; Folium]
                        J --&gt;|Shows| L[Emissions Output]
                        K &amp; L --&gt;|Final Output| M[User Dashboard]
                    end
                &lt;/div&gt;

  &lt;/div&gt;
&lt;p&gt;^ this one is really detailed. but i’m sure its taking inputs from the readme file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;trying without giving the readme file, as the readme already contains the flow.&lt;/p&gt;
&lt;p&gt;prompt: [llm ready parsed info (token length - 4.7k)] given the above description of a repository, write code for a mermaid diagram explaining the same. give 2 separate codes - one explaining the directory structure and the other explaining the code, logic and flow&lt;/p&gt;
&lt;p&gt;output:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;fixed output&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
                graph TD
                    A[1rvinn-fedexpedite]
                    A --&gt; B[app.py]
                    A --&gt; C[requirements.txt]
                    A --&gt; D[images/]
                    A --&gt; E[.streamlit/]
                    E --&gt; F[config.toml]
                &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
                flowchart TD
                    A[Start: Streamlit App Initialization]
                    B[Set page configuration &amp; initialize session state - stops, optimal_route]
                    C[Display App Logo]
                    D[Sidebar: Set Route Calculation Parameters - Traffic, etc.]
                    E[Sidebar: Select Vehicle Details - Travel Mode, Car Type, Engine Size, Fuel Type]
                    F[Sidebar: Choose Input Mode - Coordinates or Search for Destination]
                    G{Input Mode Check}
                    H[Enter Starting Location]
                    I[If Search: Call HERE API to fetch coordinates]
                    J[Add Stop - Coordinates/Search and update session state]
                    K[Optional: Input Ending Location - flexible]
                    L[Display Added Stops with option to Clear]
                    M[Click Calculate Optimal Route]
                    N[For each pair of locations: Call TomTom API to get route details]
                    O[Solve the Traveling Salesman Problem using Held-Karp Algorithm]
                    P[Generate Optimal Route Sequence, Total Distance &amp; Travel Time]
                    Q[Align stop names with the optimal sequence]
                    R[Generate Google Maps URL for the route]
                    S[Calculate Emissions based on vehicle and distance]
                    T[Display Results: Sequence, Total Time, Distance, Emissions]
                    U[Visualize Route on a Folium Map with Markers]
                    V[End]
                
                    A --&gt; B
                    B --&gt; C
                    C --&gt; D
                    D --&gt; E
                    E --&gt; F
                    F --&gt; G
                    G -- &#34;Coordinates&#34; --&gt; H
                    G -- &#34;Search&#34; --&gt; I
                    H --&gt; J
                    I --&gt; J
                    J --&gt; K
                    K --&gt; L
                    L --&gt; M
                    M --&gt; N
                    N --&gt; O
                    O --&gt; P
                    P --&gt; Q
                    Q --&gt; R
                    R --&gt; S
                    S --&gt; T
                    T --&gt; U
                    U --&gt; V
                &lt;/div&gt;

  &lt;/div&gt;
&lt;p&gt;^ this is too ass.&lt;/p&gt;
&lt;p&gt;asked to redo, gave this:&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
                graph TD;
                    A[Start - Streamlit App] --&gt;|Initialize Session| B[Session State Variables]
                    A --&gt;|Load API Keys| C[TomTom &amp; HERE API Keys]
                    A --&gt;|Set Streamlit Config| D[UI Configuration]
                
                    B --&gt; E[Sidebar Input]
                    E --&gt;|User Inputs Start Location| F[Fetch Coordinates via HERE API]
                    E --&gt;|User Inputs Stops| G[Add Stops]
                    E --&gt;|User Selects Vehicle Type| H[Emission Calculation Parameters]
                    E --&gt;|User Clicks Calculate Route| I[Calculate Optimal Route]
                
                    I --&gt; J[Calculate Route using TomTom API]
                    J --&gt; K[Solve TSP for Optimal Path]
                    K --&gt; L[Store Optimal Route in Session State]
                
                    L --&gt; M[Display Results]
                    M --&gt;|Show Route Sequence| N[Show Optimal Stops]
                    M --&gt;|Show Travel Time &amp; Distance| O[Calculate Emissions]
                    M --&gt;|Show Google Maps Link| P[Generate Google Maps URL]
                    M --&gt;|Display Route on Folium Map| Q[Render Map with Start, Stops, and End]
                
                    Q --&gt; R[End - Streamlit App]
                    
                &lt;/div&gt;

  &lt;/div&gt;
&lt;p&gt;^ still ass.&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
                graph TD;
                    A[Start - Streamlit App] --&gt;|Initialize Session| B[Session State Variables]
                    A --&gt;|Load API Keys| C[TomTom &amp; HERE API Keys]
                    A --&gt;|Set Streamlit Config| D[UI Configuration]
                
                    B --&gt; E[Sidebar Input]
                    E --&gt;|User Inputs Start Location| F[Fetch Coordinates via HERE API]
                    E --&gt;|User Inputs Stops| G[Add Stops]
                    E --&gt;|User Selects Vehicle Type| H[Emission Calculation Parameters]
                    E --&gt;|User Clicks Calculate Route| I[Calculate Optimal Route]
                
                    I --&gt; J[Calculate Route using TomTom API]
                    J --&gt; K[Solve TSP for Optimal Path]
                    K --&gt; L[Store Optimal Route in Session State]
                
                    L --&gt; M[Display Results]
                    M --&gt;|Show Route Sequence| N[Show Optimal Stops]
                    M --&gt;|Show Travel Time &amp; Distance| O[Calculate Emissions]
                    M --&gt;|Show Google Maps Link| P[Generate Google Maps URL]
                    M --&gt;|Display Route on Folium Map| Q[Render Map with Start, Stops, and End]
                
                    Q --&gt; R[End - Streamlit App]
                &lt;/div&gt;

  &lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;the outputs were sub standard despite using a sota model - 4o which has a big context window.&lt;/li&gt;
&lt;li&gt;when i use an api, the output’s quality is bound to degrade.&lt;/li&gt;
&lt;li&gt;the token size is also something that should be taken care of; some repos are excessively big and hence really big token sizes which would be hard to handle.&lt;/li&gt;
&lt;li&gt;therefore, it is of utmost importance to divide the content into sections, provide summaries for each part and then ask the model to create a diagram.
&lt;ul&gt;
&lt;li&gt;need to test this hypothesis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;getting started with the real shit now&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;code to get llm ready parsed github data&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;using requests and base 64, got the following code running that prints the contents of a github repo given its name and the owner’s name. uses github api for it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; requests
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; base64
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# GitHub repository details &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;BASE_URL &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://api.github.com/repos/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;OWNER&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;REPO&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/contents&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fetch_repo_contents&lt;/span&gt;(path&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Recursively fetches all files in the GitHub repository.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;BASE_URL&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;path&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; path &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; BASE_URL
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    headers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Authorization&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;token &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;GITHUB_TOKEN&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;} &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; GITHUB_TOKEN &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(url, headers&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;headers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;status_code &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;json()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error fetching &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;url&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;status_code&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_file_content&lt;/span&gt;(file_url):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Fetches and decodes the content of a file from GitHub.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    headers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Authorization&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;token &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;GITHUB_TOKEN&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;} &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; GITHUB_TOKEN &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(file_url, headers&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;headers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;status_code &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        file_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;json()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; file_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            content &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; base64&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b64decode(file_data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;utf-8&amp;#34;&lt;/span&gt;, errors&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ignore&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; content
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error fetching file content: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;status_code&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;traverse_repo&lt;/span&gt;(path&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Recursively traverses the GitHub repository and prints file contents.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    items &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fetch_repo_contents(path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; items:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; items:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;file&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;📄 File: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;path&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                content &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_file_content(item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;url&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--- START OF &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; ---&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                print(content[:&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;])  &lt;span style=&#34;color:#75715e&#34;&gt;# Print first 1000 chars for preview&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;--- END OF &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; ---&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;dir&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;📂 Directory: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;path&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                traverse_repo(item[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;path&amp;#34;&lt;/span&gt;])  &lt;span style=&#34;color:#75715e&#34;&gt;# Recursive call for directories&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run the script&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;📂 Fetching repository contents for &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;OWNER&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;REPO&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;...&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;traverse_repo()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;^ need to make the following changes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; need to change this to take in a url&lt;/p&gt;
&lt;p&gt;must have ‘https://github.com/’ in the beginning - a possible scope of error&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; currently, it’s also printing image and hidden files/directories’ data, which it should skip&lt;/p&gt;
&lt;p&gt;^ excluded the following extensions:&lt;/p&gt;
&lt;p&gt;.png, .jpg, .jpeg, .gif, .svg, .ico, .webp, .pdf, .zip, .tar, .gz, .mp4, .mp3, .avi&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; need it to also print the structure on top&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; must also print that it has exited a directory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;issue: github api has a limited no. of requests (60) per hour per ip.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one request is used per file/directory accessed&lt;/li&gt;
&lt;li&gt;how to bypass this!!!!!!!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update 10/3/25: lol i just had to pass my github auth token and the no of requests per hour increases from 60 to 5000&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;divide it into sections (or files); llm provides a technical summary of each section (or file); then these summaries and the structure is fed into the system to analyse the contents and give a mermaid diagram&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;can we integrate rag so that it’s able to refer to specifics within the code for a more detailed diagram.&lt;/p&gt;
&lt;p&gt;^ dont really think this is required, since the mermaid diagram is only aimed at providing a high level summary of how the entire code works.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;can have mermaid diagrams per section (or file), then one combining them too&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;this kind of ui looks really good. plus if the creation could be animated then would look very cool.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://1rvinn.github.io/img/build/mindpalace/image%206.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[24/2/25]&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;work on segmenter, summarizer, relation determiner and mermaid diag generator first.&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    flowchart 
    A[user input]---&gt;B[pdf]---&gt;|parser - ocr/text extractor|E[segmentor]
    A---&gt;C[text]---&gt;E
    A---&gt;D[url]---&gt;|parser - beautiful soup/selenium|E
    A---&gt;K[github repo]---&gt;|repo parser - graphql or gitingest|C[text]
    E---&gt;|topic-wise segmented text packets|F[summarizer]
    F---&gt;|topic-wise summarized packets|G[relationship determiner]---&gt;H---&gt;I[combined mermaid diag]
    F---&gt;|summarized text per topic|H[mermaid diagram generator]---&gt;J[individual mermaid diagrams]
    &lt;/div&gt;

  &lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;random idea but how would an ai model architecture visualizer be?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;code for ssrg:&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;iter 1&lt;/summary&gt;
    - used this code
&lt;pre&gt;&lt;code&gt;    ```python
    import os
    import re
    import google.generativeai as genai
    import json
    
    genai.configure(api_key=os.environ[&amp;quot;GEMINI_API_KEY&amp;quot;])
    
    # Create the model
    generation_config = {
        &amp;quot;temperature&amp;quot;: 1,
        &amp;quot;top_p&amp;quot;: 0.95,
        &amp;quot;top_k&amp;quot;: 40,
        &amp;quot;max_output_tokens&amp;quot;: 8192,
        &amp;quot;response_mime_type&amp;quot;: &amp;quot;text/plain&amp;quot;,
    }
    
    def extract_json(text):
        &amp;quot;&amp;quot;&amp;quot;Extracts valid JSON from Gemini API response.&amp;quot;&amp;quot;&amp;quot;
        match = re.search(r&amp;quot;\{.*\}&amp;quot;, text, re.DOTALL)  # Find JSON inside { ... }
        if match:
            return match.group(0)  # Return only the JSON part
        return None
    
    def ss(text):
        model1 = genai.GenerativeModel(
        model_name=&amp;quot;gemini-2.0-flash&amp;quot;,
        generation_config=generation_config,
        system_instruction=&amp;quot;You are an AI that extracts key topics from text and summarizes them.\n\n    - Identify the major topics.\n    - Provide a 2-3 sentence summary per topic.\n    - Return results in JSON format:\n    \n    {{\n      \&amp;quot;topics\&amp;quot;: [\n        {{\&amp;quot;topic\&amp;quot;: \&amp;quot;Topic 1\&amp;quot;, \&amp;quot;summary\&amp;quot;: \&amp;quot;Summary of topic 1\&amp;quot;}},\n        {{\&amp;quot;topic\&amp;quot;: \&amp;quot;Topic 2\&amp;quot;, \&amp;quot;summary\&amp;quot;: \&amp;quot;Summary of topic 2\&amp;quot;}}\n      ]\n    }} \n**output only JSON in plaintext format, nothing else&amp;quot;,
        )
    
        chat_session = model1.start_chat(
        history=[
        ]
        )
        response = chat_session.send_message(text)
    
        return response.text
    
    def relation(summary):
        model2 = genai.GenerativeModel(
        model_name=&amp;quot;gemini-2.0-flash&amp;quot;,
        generation_config=generation_config,
        system_instruction=&amp;quot;You are an AI that identifies relationships between topics.\n\n    Given this JSON data of topics, determine relevant relationships:\n    \n    JSON Data: {segmented_data}\n    \n    Output format:\n    {{\n      \&amp;quot;relationships\&amp;quot;: [\n        {{\&amp;quot;from\&amp;quot;: \&amp;quot;Topic 1\&amp;quot;, \&amp;quot;to\&amp;quot;: \&amp;quot;Topic 2\&amp;quot;, \&amp;quot;relationship\&amp;quot;: \&amp;quot;How they are related\&amp;quot;}},\n        {{\&amp;quot;from\&amp;quot;: \&amp;quot;Topic 2\&amp;quot;, \&amp;quot;to\&amp;quot;: \&amp;quot;Topic 3\&amp;quot;, \&amp;quot;relationship\&amp;quot;: \&amp;quot;Another connection\&amp;quot;}}\n      ]\n    }} \n**output only JSON, nothing else**&amp;quot;,
        )
    
        chat_session = model2.start_chat(
        history=[
        ]
        )
    
        response = chat_session.send_message(summary)
        return response.text
    
    def generate_mermaid_code(topics_json, relationships_json):
        topics = json.loads(topics_json)[&amp;quot;topics&amp;quot;]
        relationships = json.loads(relationships_json)[&amp;quot;relationships&amp;quot;]
    
        mermaid_code = &amp;quot;graph TD;\n&amp;quot;
    
        # Add topics as nodes
        for i, topic in enumerate(topics):
            mermaid_code += f&#39;  {chr(65+i)}[&amp;quot;{topic[&amp;quot;topic&amp;quot;]}&amp;quot;];\n&#39;
    
        # Add relationships as edges
        for relation in relationships:
            from_topic = relation[&amp;quot;from&amp;quot;]
            to_topic = relation[&amp;quot;to&amp;quot;]
            relation_text = relation[&amp;quot;relationship&amp;quot;]
    
            from_id = next(i for i, t in enumerate(topics) if t[&amp;quot;topic&amp;quot;] == from_topic)
            to_id = next(i for i, t in enumerate(topics) if t[&amp;quot;topic&amp;quot;] == to_topic)
    
            mermaid_code += f&#39;  {chr(65+from_id)} --&amp;gt;|{relation_text}| {chr(65+to_id)};\n&#39;
    
        return mermaid_code
    
    # def generate_mind_map(text):
    #     segmented_data = ss(text)
    #     relationships_data = relation(segmented_data)
    #     mermaid_diagram = generate_mermaid_code(segmented_data, relationships_data)
    
    #     return mermaid_diagram
    
    if __name__ == &amp;quot;__main__&amp;quot;:
        input_text = input(&amp;quot;input: &amp;quot;)
        
        segmented_data = extract_json(ss(input_text))
        print(f&amp;quot;1: {segmented_data}&amp;quot;)
        relationships_data = extract_json(relation(segmented_data))
        print(f&amp;quot;w: {relationships_data}&amp;quot;)
        mermaid_diagram = generate_mermaid_code(segmented_data, relationships_data)
    
        print(&amp;quot;Generated Mermaid Diagram:\n&amp;quot;)
        print(mermaid_diagram)
    ```
    
    input:
    
    ```
    Artificial Intelligence is revolutionizing industries.
    In healthcare, AI assists with diagnostics and personalized treatments.
    In finance, AI helps detect fraud and optimize trading strategies.
    Machine learning models power these applications, improving accuracy.
    ```
    
    output:
    
    &amp;lt;div style=&#39;text-align:center;&#39;&amp;gt;
    &lt;div class=&#34;mermaid&#34;&gt;
        graph TD;
            A[&#34;AI Revolutionizing Industries&#34;];
            B[&#34;AI in Healthcare&#34;];
            C[&#34;AI in Finance&#34;];
            B --&gt;|AI in Healthcare is a specific application area contributing to the broader AI Revolution across various industries.| A;
            C --&gt;|AI in Finance is a specific application area contributing to the broader AI Revolution across various industries.| A;
            B --&gt;|Both topics represent specific examples of how AI is applied in different sectors to improve efficiency and decision-making through data analysis.| C;
        &lt;/div&gt;

    &amp;lt;/div&amp;gt;
    
    thoughts:
    
    - it identifies major topics well
    - relationships are mid
    - there is no particular flow to it
- prompts
    1. summarizer and segmentor

&amp;lt;div style=&#39;text-align:center;&#39;&amp;gt;
&lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Introduction and Applicability&#34;];
        B[&#34;General Controls: Governance and Management of Security Risks&#34;];
        A --&gt;|The &#39;Introduction and Applicability&#39; section sets the scope and effective date for the directions, while the &#39;General Controls&#39; section outlines the specific governance and management policies that Regulated Entities - REs - must implement to comply with these directions.| B;
    &lt;/div&gt;

&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;iter 2: changed the prompts a bit to increase no. of nodes and better connections&lt;/summary&gt;
        - new prompts
            - segmentor and summarizer:
&lt;pre&gt;&lt;code&gt;            ```
            Extract as many **short, precise** topics as possible from the given text.
            
                **Rules:**
                - Each topic must be **concise (1-3 words max)**.
                - Aim for **10+ topics** if the content allows.
                - Summarize each topic in **one short sentence**.
                - **Strictly return JSON format only.**
            
                **Example Output:**
                ```json
                {{
                  &amp;quot;topics&amp;quot;: [
                    {{&amp;quot;topic&amp;quot;: &amp;quot;AI in Medicine&amp;quot;, &amp;quot;summary&amp;quot;: &amp;quot;AI enhances diagnostics and treatment&amp;quot;}},
                    {{&amp;quot;topic&amp;quot;: &amp;quot;Fraud Detection&amp;quot;, &amp;quot;summary&amp;quot;: &amp;quot;AI identifies financial fraud in real-time&amp;quot;}},
                    {{&amp;quot;topic&amp;quot;: &amp;quot;Robotics&amp;quot;, &amp;quot;summary&amp;quot;: &amp;quot;Autonomous robots optimize industrial tasks&amp;quot;}},
                    {{&amp;quot;topic&amp;quot;: &amp;quot;Self-Driving Cars&amp;quot;, &amp;quot;summary&amp;quot;: &amp;quot;AI enables autonomous vehicle navigation&amp;quot;}}
                  ]
                }}
                ```
            
                **Text Input:** {text}
            ```
            
        - relationship determiner
            
            ```
            Identify **diverse** relationships between the given topics.
            
                **Rules:**
                - Find as many **relevant connections** as possible.
                - Use **short, precise** relationship descriptions.
                - Relationships should not be **too obvious** (avoid generic links).
                - **Strictly return JSON format only.**
            
                **Example Output:**
                ```json
                {{
                  &amp;quot;relationships&amp;quot;: [
                    {{&amp;quot;from&amp;quot;: &amp;quot;AI in Medicine&amp;quot;, &amp;quot;to&amp;quot;: &amp;quot;Machine Learning&amp;quot;, &amp;quot;relationship&amp;quot;: &amp;quot;Uses ML models&amp;quot;}},
                    {{&amp;quot;from&amp;quot;: &amp;quot;Self-Driving Cars&amp;quot;, &amp;quot;to&amp;quot;: &amp;quot;Computer Vision&amp;quot;, &amp;quot;relationship&amp;quot;: &amp;quot;Relies on CV for object detection&amp;quot;}},
                    {{&amp;quot;from&amp;quot;: &amp;quot;Fraud Detection&amp;quot;, &amp;quot;to&amp;quot;: &amp;quot;AI in Finance&amp;quot;, &amp;quot;relationship&amp;quot;: &amp;quot;Prevents financial crimes&amp;quot;}},
                    {{&amp;quot;from&amp;quot;: &amp;quot;Robotics&amp;quot;, &amp;quot;to&amp;quot;: &amp;quot;AI Ethics&amp;quot;, &amp;quot;relationship&amp;quot;: &amp;quot;Raises ethical concerns&amp;quot;}}
                  ]
                }}
                ```
            
                **Given Topic Data:**  
                ```json
                {segmented_data}
                ```
            
                **Output only JSON. No extra text.**
            ```
            
    - result
        
        &amp;lt;div style=&#39;text-align:center;&#39;&amp;gt;
        &lt;div class=&#34;mermaid&#34;&gt;
            graph TD;
              A[&#34;Digital Payments&#34;];
              B[&#34;Regulatory Framework&#34;];
              C[&#34;Effective Date&#34;];
              D[&#34;Regulated Entities&#34;];
              E[&#34;Definitions&#34;];
              F[&#34;Security Risks&#34;];
              G[&#34;Board Approval&#34;];
              H[&#34;Customer Data&#34;];
              I[&#34;System Availability&#34;];
              J[&#34;Dispute Resolution&#34;];
              K[&#34;Policy Review&#34;];
              L[&#34;User Acceptance Tests&#34;];
              M[&#34;External Assessment&#34;];
              A --&gt;|Subject to various security threats| F;
              A --&gt;|Governed by legal guidelines| B;
              B --&gt;|Defines the scope of entities covered| D;
              C --&gt;|Determines when the legal framework becomes active| B;
              E --&gt;|Provides clarity to the legal terms used| B;
              F --&gt;|Threats can compromise customer information| H;
              F --&gt;|Can lead to service outages| I;
              G --&gt;|Needed for key decisions related to digital payments| A;
              H --&gt;|May be needed to resolve disputes| J;
              I --&gt;|Critical for reliable digital payment services| A;
              K --&gt;|Ensures alignment with evolving legal standards| B;
              L --&gt;|Validates usability before payment platform launch| A;
              M --&gt;|Identifies vulnerabilities and mitigates risks| F;
              G --&gt;|Required for changes to digital payment policies| K;
              M --&gt;|Ensures security of digital payment systems| A;
              D --&gt;|Must adhere to the guidelines for digital payments| A;
            &lt;/div&gt;

        &amp;lt;/div&amp;gt;
        
        &amp;lt;div style=&#39;text-align:center;&#39;&amp;gt;
        &lt;div class=&#34;mermaid&#34;&gt;
            graph TD;
              A[&#34;Graph Theory&#34;];
              B[&#34;TSP&#34;];
              C[&#34;Optimization&#34;];
              D[&#34;Dijkstra&#39;s Algorithm&#34;];
              E[&#34;Held-Karp&#34;];
              F[&#34;Hackathon&#34;];
              G[&#34;Route Optimization&#34;];
              H[&#34;Real-Time Traffic&#34;];
              I[&#34;Google Maps&#34;];
              J[&#34;Streamlit&#34;];
              K[&#34;Web Application&#34;];
              L[&#34;Simplicity&#34;];
              M[&#34;Product Focus&#34;];
              N[&#34;Functionality&#34;];
              A --&gt;|Provides the foundation for representing TSP problems| B;
              B --&gt;|A classic optimization problem| C;
              D --&gt;|Algorithm operating on graphs| A;
              E --&gt;|Dynamic programming approach to solve TSP| B;
              F --&gt;|Context for developing route optimization solutions| G;
              G --&gt;|Can incorporate real-time traffic data for dynamic adjustments| H;
              G --&gt;|Uses maps for calculating efficient routes| I;
              K --&gt;|Built using the Streamlit framework| J;
              F --&gt;|Deliverable of the hackathon| K;
              G --&gt;|May be a practical application or extension of solving TSP| B;
              H --&gt;|Supplement maps data for navigation| I;
              K --&gt;|Focus on core functions over superfluous features| N;
              F --&gt;|Simplicity was a key winning factor| L;
              M --&gt;|Essential for success| F;
              H --&gt;|Adds complexity to route optimization| C;
              B --&gt;|Dijkstra&#39;s algorithm can be used in heuristics to solve TSP| D;
              A --&gt;|Graph theory provides tools used in optimization problems| C;
              G --&gt;|A route optimization feature can be included in the web application| K;
              M --&gt;|Focusing on core functionality is key| N;
              B --&gt;|TSP concepts are used in route optimization| G;
              H --&gt;|Improves route quality in real-time| G;
              N --&gt;|Often related aspects of the same solution| L;
              F --&gt;|Requires product understanding| M;
              K --&gt;|Integrates mapping services| I;
              L --&gt;|Simple design can enhance functionality| N;
              J --&gt;|Tool for building front-end interfaces| K;
              D --&gt;|One possible algorithm for route calculation| G;
              C --&gt;|The goal is to achieve the optimal path| G;
              N --&gt;|Core feature of a product| M;
            &lt;/div&gt;

        &amp;lt;/div&amp;gt;
        
    - thoughts:
        - looks really good
        - however i think the number of connections is too much
        - flow is hard to understand
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;iter 3: tweaked the prompts to give a nice flow and reduce the number of connections&lt;/summary&gt;
&lt;p&gt;changed rel to:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;f&amp;#34;&amp;#34;&amp;#34;
    **Task:** Organize topics into a clear mind map with a structured flow.

    **Rules:**
    - Identify a **main starting topic** (e.g., &amp;#34;AI Applications&amp;#34; or &amp;#34;Machine Learning&amp;#34;).
    - Group related topics under **subcategories** (e.g., &amp;#34;AI in Healthcare&amp;#34; under &amp;#34;AI Applications&amp;#34;).
    - Ensure **a natural progression** from general → specific.
    - Limit connections to **meaningful** ones. Avoid excessive cross-links.
    - **Strictly return JSON format only.**

    **Example Output:**
    ```json
    {{
        &amp;#34;root&amp;#34;: &amp;#34;Artificial Intelligence&amp;#34;,
        &amp;#34;relationships&amp;#34;: [
        {{&amp;#34;from&amp;#34;: &amp;#34;Artificial Intelligence&amp;#34;, &amp;#34;to&amp;#34;: &amp;#34;AI Applications&amp;#34;, &amp;#34;relationship&amp;#34;: &amp;#34;Broad category&amp;#34;}},
        {{&amp;#34;from&amp;#34;: &amp;#34;AI Applications&amp;#34;, &amp;#34;to&amp;#34;: &amp;#34;AI in Healthcare&amp;#34;, &amp;#34;relationship&amp;#34;: &amp;#34;A major domain&amp;#34;}},
        {{&amp;#34;from&amp;#34;: &amp;#34;AI Applications&amp;#34;, &amp;#34;to&amp;#34;: &amp;#34;AI in Finance&amp;#34;, &amp;#34;relationship&amp;#34;: &amp;#34;Used in banking&amp;#34;}},
        {{&amp;#34;from&amp;#34;: &amp;#34;AI in Healthcare&amp;#34;, &amp;#34;to&amp;#34;: &amp;#34;Medical Diagnosis&amp;#34;, &amp;#34;relationship&amp;#34;: &amp;#34;Key use case&amp;#34;}},
        {{&amp;#34;from&amp;#34;: &amp;#34;AI in Finance&amp;#34;, &amp;#34;to&amp;#34;: &amp;#34;Fraud Detection&amp;#34;, &amp;#34;relationship&amp;#34;: &amp;#34;Security application&amp;#34;}}
        ]
    }}
    ```

    **Given Topics:**
    ```json
    {summary}
    ```

    **Output only JSON. No extra text.**
    &amp;#34;&amp;#34;&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;output&lt;/p&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Graph Theory&#34;];
        B[&#34;TSP&#34;];
        C[&#34;Optimization&#34;];
        D[&#34;Algorithms&#34;];
        E[&#34;Hackathon&#34;];
        F[&#34;Route Optimization&#34;];
        G[&#34;Real-time Traffic&#34;];
        H[&#34;APIs&#34;];
        I[&#34;Streamlit&#34;];
        J[&#34;Web Application&#34;];
        K[&#34;Simplicity&#34;];
        L[&#34;Product Focus&#34;];
        M[&#34;Efficacy&#34;];
        F --&gt;|Relies on| C;
        F --&gt;|Implements| D;
        D --&gt;|Utilizes concepts from| A;
        D --&gt;|Solves| B;
        F --&gt;|Data source| H;
        H --&gt;|Provides| G;
        F --&gt;|Interface is| J;
        J --&gt;|Built with| I;
        F --&gt;|Developed during| E;
        F --&gt;|Driven by| L;
        L --&gt;|Emphasizes| K;
        L --&gt;|Aims for| M;
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        Graph_Theory[&#34;Graph Theory&#34;];
        TSP[&#34;TSP&#34;];
        Optimization[&#34;Optimization&#34;];
        Dijkstra&#39;s_Algorithm[&#34;Dijkstra&#39;s Algorithm&#34;];
        Held-Karp[&#34;Held-Karp&#34;];
        Hackathon[&#34;Hackathon&#34;];
        Route_Optimization[&#34;Route Optimization&#34;];
        Real-time_Traffic[&#34;Real-time Traffic&#34;];
        Emission_Data[&#34;Emission Data&#34;];
        APIs[&#34;APIs&#34;];
        Streamlit[&#34;Streamlit&#34;];
        Web_Application[&#34;Web Application&#34;];
        Simplicity[&#34;Simplicity&#34;];
        Product_Focus[&#34;Product Focus&#34;];
        Web_Application --&gt;|Frontend framework| Streamlit;
        Route_Optimization --&gt;|Based on| Graph_Theory;
        Route_Optimization --&gt;|Uses mathematical| Optimization;
        Route_Optimization --&gt;|Related to| TSP;
        TSP --&gt;|Algorithm| Dijkstra&#39;s_Algorithm;
        TSP --&gt;|Algorithm| Held-Karp;
        Route_Optimization --&gt;|Considers| Real-time_Traffic;
        Route_Optimization --&gt;|Calculates| Emission_Data;
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Graph Theory&#34;];
        B[&#34;TSP&#34;];
        C[&#34;Optimization&#34;];
        D[&#34;Dijkstra&#39;s Algorithm&#34;];
        E[&#34;Held-Karp Algorithm&#34;];
        F[&#34;Hackathon&#34;];
        G[&#34;Route Optimization&#34;];
        H[&#34;Real-time Traffic&#34;];
        I[&#34;APIs&#34;];
        J[&#34;Streamlit&#34;];
        K[&#34;User Experience&#34;];
        L[&#34;Simplicity&#34;];
        M[&#34;Product&#34;];
        N[&#34;Functionality&#34;];
        O[&#34;Steve Jobs&#34;];
        C --&gt;|Encompasses the TSP problem| B;
        A --&gt;|Provides the foundation for| B;
        D --&gt;|Can be a component of| G;
        E --&gt;|Offers a solution approach| B;
        H --&gt;|Influences dynamically| G;
        G --&gt;|Was the challenge at the| F;
        I --&gt;|Delivers real-time data| H;
        I --&gt;|Provides data for| G;
        J --&gt;|Aids in creating intuitive| K;
        K --&gt;|Enhanced by| L;
        M --&gt;|Prioritizes robust| N;
        N --&gt;|Complements clean| L;
        F --&gt;|Offered a lesson in| M;
        O --&gt;|Is an inspiration for| M;
        I --&gt;|Critical component used during| F;
    
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Graph Theory&#34;];
        B[&#34;TSP&#34;];
        C[&#34;Dijkstra&#39;s Algorithm&#34;];
        D[&#34;Held-Karp Algorithm&#34;];
        E[&#34;FedEx Hackathon&#34;];
        F[&#34;Real-time Traffic&#34;];
        G[&#34;Search API&#34;];
        H[&#34;Navigation API&#34;];
        I[&#34;Streamlit&#34;];
        J[&#34;Web Application&#34;];
        K[&#34;Simplicity&#34;];
        L[&#34;Product Understanding&#34;];
        A --&gt;|Provides foundation for| B;
        B --&gt;|Alternative solution approach - limited| C;
        B --&gt;|Solves optimally for smaller instances| D;
        E --&gt;|Inspired a real-world application of| B;
        E --&gt;|Resulted in a| J;
        J --&gt;|Built frontend using| I;
        J --&gt;|Utilized for route guidance| H;
        J --&gt;|Integrated for location finding| G;
        H --&gt;|Considers for optimal routing| F;
        E --&gt;|API provided data| F;
        E --&gt;|Winning strategy emphasized| K;
        E --&gt;|Essential for effective solution| L;
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Graph Theory&#34;];
        B[&#34;TSP&#34;];
        C[&#34;Optimization&#34;];
        D[&#34;Dijkstra&#39;s Algorithm&#34;];
        E[&#34;Held-Karp&#34;];
        F[&#34;FedEx Hackathon&#34;];
        G[&#34;Route Optimization&#34;];
        H[&#34;Real-Time Data&#34;];
        I[&#34;API Integration&#34;];
        J[&#34;Streamlit&#34;];
        K[&#34;Web Application&#34;];
        L[&#34;Simplicity&#34;];
        M[&#34;Product Focus&#34;];
        N[&#34;Functionality&#34;];
        B --&gt;|Is a problem within| A;
        C --&gt;|Seeks solutions through| B;
        D --&gt;|Is an algorithm within| A;
        D --&gt;|Can be a component of| G;
        E --&gt;|Solves optimally using dynamic programming| B;
        G --&gt;|Applies principles of| C;
        F --&gt;|Focuses on problems related to| G;
        H --&gt;|Enhances accuracy of| G;
        I --&gt;|Provides sources for| H;
        I --&gt;|Supplies data and services for| G;
        J --&gt;|Facilitates rapid development of| K;
        K --&gt;|The deliverable for| F;
        K --&gt;|Interface for displaying and managing| G;
        L --&gt;|A design principle applied to| K;
        L --&gt;|Prioritized over complexity| N;
        M --&gt;|Drives the user-centered design of| K;
        M --&gt;|Guides the selection of features based on user needs| N;
        N --&gt;|Key element for success in| F;
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Digital Payments&#34;];
        B[&#34;Regulation&#34;];
        C[&#34;Applicability&#34;];
        D[&#34;Policy Formulation&#34;];
        E[&#34;Security Risks&#34;];
        F[&#34;Customer Data&#34;];
        G[&#34;Infrastructure&#34;];
        H[&#34;Product Security&#34;];
        I[&#34;Scalability&#34;];
        J[&#34;Dispute Resolution&#34;];
        K[&#34;Review Mechanism&#34;];
        L[&#34;User Acceptance Tests&#34;];
        M[&#34;External Assessment&#34;];
        A --&gt;|Governed by regulatory directions| B;
        B --&gt;|Specifies scope of rules| C;
        A --&gt;|Requires regulated entities to create policies| D;
        D --&gt;|Addresses governance and management of risks| E;
        D --&gt;|Mandates confidentiality protection| F;
        D --&gt;|Ensures availability of infrastructure| G;
        D --&gt;|Focuses on building secure payment products| H;
        D --&gt;|Incorporates capacity building| I;
        D --&gt;|Ensures efficient handling of disputes| J;
        D --&gt;|Includes processes for adequate review| K;
        H --&gt;|Requires UAT before launch| L;
        H --&gt;|Needs external validation| M;
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Gitingest&#34;];
        B[&#34;CLI Tool&#34;];
        C[&#34;Python Package&#34;];
        D[&#34;Browser Extension&#34;];
        E[&#34;Self-Hosting&#34;];
        F[&#34;Contributing&#34;];
        G[&#34;Code of Conduct&#34;];
        H[&#34;Security Policy&#34;];
        I[&#34;pyproject.toml&#34;];
        J[&#34;requirements.txt&#34;];
        K[&#34;pre-commit&#34;];
        L[&#34;Filesystem schema&#34;];
        M[&#34;Query parsing&#34;];
        N[&#34;Output formatting&#34;];
        O[&#34;Server API&#34;];
        P[&#34;GitHub Actions&#34;];
        A --&gt;|Exposes functionality through| B;
        A --&gt;|Provides as an importable| C;
        A --&gt;|Offers for code analysis in browsers| D;
        A --&gt;|Supports via Docker| E;
        A --&gt;|Welcomes contributions to| F;
        A --&gt;|Governed by| G;
        A --&gt;|Maintains a| H;
        A --&gt;|Automates CI/CD with| P;
        B --&gt;|Uses for processing input sources| M;
        B --&gt;|Utilizes for presenting results| N;
        C --&gt;|Uses for processing input sources| M;
        C --&gt;|Utilizes for presenting results| N;
        E --&gt;|Accesses through a| O;
        F --&gt;|Adheres to| G;
        F --&gt;|Uses for testing contributions| P;
        A --&gt;|Uses to manage project metadata| I;
        A --&gt;|Defines Python dependencies in| J;
        A --&gt;|Enforces code quality with| K;
        A --&gt;|Operates based on a defined| L;
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Gitingest Tool&#34;];
        B[&#34;Code of Conduct&#34;];
        C[&#34;Contributing&#34;];
        D[&#34;Dockerfile&#34;];
        E[&#34;MIT License&#34;];
        F[&#34;Security Policy&#34;];
        G[&#34;CLI Interface&#34;];
        H[&#34;Repository Cloning&#34;];
        I[&#34;Configuration&#34;];
        J[&#34;Exception Handling&#34;];
        K[&#34;File System Schema&#34;];
        L[&#34;Ingestion Process&#34;];
        M[&#34;Output Formatting&#34;];
        N[&#34;Query Parsing&#34;];
        O[&#34;Server Main&#34;];
        P[&#34;FastAPI Server&#34;];
        Q[&#34;Query Processing&#34;];
        R[&#34;Server Utilities&#34;];
        S[&#34;Web Application&#34;];
        T[&#34;Github Actions&#34;];
        A --&gt;|Offers command-line access| G;
        A --&gt;|Provides API endpoints| P;
        A --&gt;|Core analysis functionality| L;
        A --&gt;|Usage is governed by| E;
        A --&gt;|Project adheres to| B;
        A --&gt;|Follows guidelines for contributions| C;
        A --&gt;|Provides details for reporting vulnerabilities| F;
        G --&gt;|Uses for input validation| N;
        G --&gt;|Reads settings from| I;
        P --&gt;|Entry point for| O;
        P --&gt;|Manages backend requests| Q;
        P --&gt;|Utilizes helper methods| R;
        P --&gt;|Serves frontend components| S;
        L --&gt;|Requires access to codebases| H;
        L --&gt;|Represents file structures| K;
        L --&gt;|Generates analysis results| M;
        L --&gt;|Reports processing errors| J;
        A --&gt;|Containerized using| D;
        A --&gt;|Uses for CI/CD| T;
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;FedEx Route Planner&#34;];
        B[&#34;Streamlit App&#34;];
        C[&#34;TomTom API&#34;];
        D[&#34;HERE API&#34;];
        E[&#34;Traffic Data&#34;];
        F[&#34;Emission Factors&#34;];
        G[&#34;Travel Mode&#34;];
        H[&#34;Input Mode&#34;];
        I[&#34;Optimal Route&#34;];
        J[&#34;TSP Solving&#34;];
        K[&#34;Route Visualization&#34;];
        L[&#34;Emission Calculation&#34;];
        A --&gt;|Implemented as| B;
        A --&gt;|Aims to find| I;
        A --&gt;|Integrates| L;
        I --&gt;|Achieved through| J;
        I --&gt;|Uses for route generation| C;
        I --&gt;|Considers for route adjustments| E;
        L --&gt;|Uses for estimation| F;
        L --&gt;|Depends on| G;
        B --&gt;|Offers configuration via| H;
        B --&gt;|Provides user interface for| K;
        H --&gt;|Uses for location to coordinate conversion| D;
        K --&gt;|Employs for map display| C;
        C --&gt;|Provides real-time| E;
    &lt;/div&gt;

  &lt;/div&gt;
  &lt;div style=&#39;text-align:center;&#39;&gt;
  &lt;div class=&#34;mermaid&#34;&gt;
    graph TD;
        A[&#34;Public-Private Partnership&#34;];
        B[&#34;Sports Sector&#34;];
        C[&#34;Odisha State&#34;];
        D[&#34;Economic Growth&#34;];
        E[&#34;Policy Implementation&#34;];
        F[&#34;Sports Development&#34;];
        G[&#34;Tokyo Olympics&#34;];
        H[&#34;Literature Review&#34;];
        I[&#34;Research Method&#34;];
        J[&#34;Global Perspectives&#34;];
        K[&#34;Indian Context&#34;];
        L[&#34;Infrastructure&#34;];
        M[&#34;Transportation&#34;];
        N[&#34;Healthcare&#34;];
        O[&#34;Education &amp; Technology&#34;];
        P[&#34;Odisha Model&#34;];
        Q[&#34;High-Performance Centers&#34;];
        R[&#34;Managerial Implications&#34;];
        S[&#34;Legal Clearances&#34;];
        T[&#34;Private Capital&#34;];
        U[&#34;Goal Alignment&#34;];
        A --&gt;|Increasingly used for infrastructure and development| B;
        A --&gt;|Contributes to sectoral growth and athlete performance| D;
        A --&gt;|Managerial implications for successful implementation| E;
        A --&gt;|Informed by existing research across sectors| H;
        A --&gt;|Models exist in Europe, North America, and South America| J;
        A --&gt;|Used in infrastructure, transportation, tourism, education, and healthcare| K;
        B --&gt;|Crucial for achieving national sporting ambitions| F;
        B --&gt;|Serves as a role model for PPP implementation| C;
        B --&gt;|India&#39;s success demonstrates the need for PPP in sports| G;
        E --&gt;|Emphasizes legal clearances, capital access, goal alignment, and project management| R;
        R --&gt;|Requires a robust framework for private organizations to plan risks| S;
        R --&gt;|Benefits from transparency to increase investment confidence| T;
        R --&gt;|Relies on understanding roles for project success| U;
        K --&gt;|PPP faces challenges like regulatory issues and capital raising| L;
        K --&gt;|PPP projects experience less time overrun| M;
        K --&gt;|PPP improves delivery in backward districts| N;
        K --&gt;|PPP reduces financial pressure on government-run schools| O;
        C --&gt;|Sports policy aims to develop all socio-economic groups| P;
        C --&gt;|Established HPCs in various sporting disciplines| Q;
        I --&gt;|Utilizes academic databases and government documents to gather existing research| H;
    &lt;/div&gt;

  &lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;p&gt;thoughts: i think these final prompts are good.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;combined thing&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;iter1:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;made a streamlit front end&lt;/li&gt;
&lt;li&gt;takes in pdfs&lt;/li&gt;
&lt;li&gt;checks whether an image based pdf or text based&lt;/li&gt;
&lt;li&gt;uses easyocr and pypdf2 for image and text based pdfs respectively&lt;/li&gt;
&lt;li&gt;extracted text given to segmentor and summarizer&lt;/li&gt;
&lt;li&gt;gives topics and summaries, displayed there&lt;/li&gt;
&lt;li&gt;passed to relationship generator. generates relationships within each thing. prints them&lt;/li&gt;
&lt;li&gt;one combined relationship generator. topic wise summaries given together, gives the relationships and prints the mermaid diagram&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;// problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;shows syntax error many times&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;// now need to change it to give the content to relationship generator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;iter2: content goes to the relationship generator. the topics are decided on the basis of major topics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;iter3: segments on the basis of titles, gives content to both rel and summarizer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;change summarize prompt&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;iter4: segments on its own, gives content to both rel and summarizer&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;there are 4 options available&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;segment and summarize together, ie the model decides the main topics. send topic wise summaries to relationship maker&lt;/li&gt;
&lt;li&gt;segment on the basis of titles, and then summarize, ie the topics are decided on the basis of how they’re presented in the doc then send topic wise summaries&lt;/li&gt;
&lt;li&gt;segment on the basis of titles, send content to summarizer to display summaries. relationship maker also gets content.&lt;/li&gt;
&lt;li&gt;segment according to what it thinks the major topics are, then send content to summarizer and relationship maker.
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;content to rel&lt;/th&gt;
          &lt;th&gt;no content to rel&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;titles&lt;/td&gt;
          &lt;td&gt;iter3&lt;/td&gt;
          &lt;td&gt;iter2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;own&lt;/td&gt;
          &lt;td&gt;iter4&lt;/td&gt;
          &lt;td&gt;iter1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;benefits of extracting titles on its own are that big files get segregated into small topics that are easy to understand.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;failed to generate topics error cuz “s, \escape in json, expecting delimiter etc
&lt;ul&gt;
&lt;li&gt;either remove these problem creating elements or donot extract content.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;because of the reason listed above, we get rid of iter3, 4.&lt;/p&gt;
&lt;p&gt;now in iter2, 1, i think it is more relevant if we do it file wise for github repos and otherwise for other pdfs.&lt;/p&gt;
&lt;p&gt;iter 1 final&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;added the functionality to take in github repos&lt;/li&gt;
&lt;li&gt;changed the prompts to be different for pdfs and repos&lt;/li&gt;
&lt;li&gt;need to make the following fixes:
&lt;ul&gt;
&lt;li&gt;syntax error&lt;/li&gt;
&lt;li&gt;mindmaps not viewable nicely&lt;/li&gt;
&lt;li&gt;ui/ux changes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;[17/3/25]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;integrated mistral’s ocr&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;omg it looks so good, i changed text based pdfs to also go through mistrals ocr&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;cleaned the code, divided into various files and enabled the functions to be imported from there&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;del&gt;need to make ui changes&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;need to create .env&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;run the prompts through chatgpt and get better ones&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;need to structure json&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;need to integrate caching&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;[18/3/25]&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the mermaid diagrams arent being represented nicely, they’re usually being cut off at the bottom unless i make the container size atypically large, which makes it very out of place.
&lt;ul&gt;
&lt;li&gt;tried numerous work arounds, nothing seemed to work&lt;/li&gt;
&lt;li&gt;then randomly came across a python library that helps to display mermaid code in streamlit and it worked like a charm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;trying to get better prompts
&lt;ul&gt;
&lt;li&gt;explained the pipeline to chatgpt, gave current prompts and asked it to generate newer and more precise ones&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;del&gt;need to improve the colour scheme&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;made prompt changes, ui changes, looks really cool. just need to integrate rag now.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/AIAnytime/Project-WebSage&#34;&gt;Project-WebSage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;^ rag based llm integration for website search&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;[26/3/25]&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;deployed on streamlit&lt;/li&gt;
&lt;li&gt;looks so fking good; i can access it through my phone, any of my browsers, literally anywhere and the best part is taht it comes with batteries included- no need to put any api keys &amp;lt;333&lt;/li&gt;
&lt;li&gt;fixes
&lt;ul&gt;
&lt;li&gt;&lt;del&gt;api keys should be locally sourced&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;the embeddings stay the same despite a new repo been given&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;the embeddings remain same across different devices - the same repo was being referred to on my phone for llm chat despite a different pdf that i had entered initially.&lt;/del&gt;
&lt;ul&gt;
&lt;li&gt;changed such that new embeddings are generated each time a file is given&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;del&gt;still shows the CachedWidgetWarning&lt;/del&gt;
&lt;ul&gt;
&lt;li&gt;put the widget generate outside the cached function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;syntax error&lt;/li&gt;
&lt;li&gt;&lt;del&gt;not loading ask ai for pdfs&lt;/del&gt;
&lt;ul&gt;
&lt;li&gt;fixed it by resetting variables whenever the input mode is changed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;update readme&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;[27/3/25]&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;del&gt;minor fixes&lt;/del&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; change size of chat dialog&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; change k factor&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; change prompt to ignore useless files&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;final-pipeline&#34;&gt;final pipeline:&lt;/h4&gt;
&lt;div style=&#39;text-align:center;&#39;&gt;
&lt;div class=&#34;mermaid&#34;&gt;
flowchart TD;
A[user input]--&gt;|&#34;github repo&#34;|B[github_scraper.py]
A --&gt; |&#34;pdf document&#34;|C[pdf_ocr.py]
B--&gt;|&#34;content&#34;|H
C--&gt;|&#34;content&#34;|H
H[&#34;content json&#34;]--&gt;D[segmentor_summarizer.py]
H--&gt;|embeddings|I[vector store]
I--&gt;|&#34;context&#34;|J[&#34;ask ai rag&#34;]
D--&gt;|&#34;major topics and summaries&#34;|E[relationship_generator.py]
E--&gt;|&#34;relationships json&#34;|F[generator.py]
F--&gt;|mermaid code|G[streamlit frontend]
&lt;/div&gt;

&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;finally. its all done. finally. got it ready. up and running, up and running beautifully. on the web. anybody from any corner of the world can use it anywhere at any corner of the world. no need for api keys, no need for downloads. just one click and you enter your mindpalace.&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;launched it on linkedin. put it on a bunch of subreddits and also on hacker news. got a bunch of positive reviews, including those from my close friends. (added more on my learnings from mindpalace in the 9/4/25 blog &lt;a href=&#34;https://1rvinn.github.io/crude/090425&#34;&gt;here.&lt;/a&gt;)&lt;/p&gt;
&lt;iframe src=&#34;https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:7311414641471983616?collapsed=1&#34; height=&#34;551&#34; width=&#34;504&#34; frameborder=&#34;0&#34; allowfullscreen=&#34;&#34; title=&#34;Embedded post&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;strong&gt;update [8/4/25]:&lt;/strong&gt; &lt;br&gt;
&lt;br&gt;
on a friend&amp;rsquo;s feedback, i added the functionality of saving the mindmaps as images. used the &lt;a href=&#34;https://kroki.io/&#34;&gt;kroki&lt;/a&gt; api endpoint to convert mermaid code into pngs and made them available for download.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>privacy, democratization in ai</title>
      <link>https://1rvinn.github.io/crude/fed/</link>
      <pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://1rvinn.github.io/crude/fed/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;recently, while reading about the recent developments in the field i realised an array of issues that, in my opinion, are being overlooked in the current paradigm:&lt;/p&gt;&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;monopolisation&lt;/strong&gt; &lt;br&gt;
Due to the humongous data requirement for pre-training, we see big-data companies like the &amp;lsquo;Googles&amp;rsquo; and &amp;lsquo;Microsofts&amp;rsquo; of the world having an unfair competitive edge over other smaller corporations or entities in developing such models. This not only means a handful of companies have control over such a quintessential field but also implies:
&lt;br&gt;
[D] soft power - greater soft power over all AI-based content, which could further transcend into propagating bias through content and controlling widespread opinions about sensitive topics across the entire world.
&lt;br&gt;
[P] access to user data - with the advent of time, as AI becomes an indispensable part of our lives, these handful corporations will have even greater access to user data across different use cases, making them even more powerful. Not only this but these days, the most popular LLMs use consumer data for further training, while only mentioning this very subtly in their privacy policy. This makes room for a plethora of data security leaks on the cloud where this data is hosted and other data attacks such as prompt injection.
&lt;br&gt;
[D] by virtue of huge data and state-of-the-art compute requirements, it is only the firms with a rich history in tech and high levels of disposable income to spend, who can establish themselves in this area, making it discriminatorily hard for smaller firms to emerge in this field. This also means a geopolitical issue for 2nd and 3rd world countries being completely dependent upon the 1st world for such foundational models.&lt;/p&gt;</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;recently, while reading about the recent developments in the field i realised an array of issues that, in my opinion, are being overlooked in the current paradigm:&lt;/p&gt;&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;monopolisation&lt;/strong&gt; &lt;br&gt;
Due to the humongous data requirement for pre-training, we see big-data companies like the &amp;lsquo;Googles&amp;rsquo; and &amp;lsquo;Microsofts&amp;rsquo; of the world having an unfair competitive edge over other smaller corporations or entities in developing such models. This not only means a handful of companies have control over such a quintessential field but also implies:
&lt;br&gt;
[D] soft power - greater soft power over all AI-based content, which could further transcend into propagating bias through content and controlling widespread opinions about sensitive topics across the entire world.
&lt;br&gt;
[P] access to user data - with the advent of time, as AI becomes an indispensable part of our lives, these handful corporations will have even greater access to user data across different use cases, making them even more powerful. Not only this but these days, the most popular LLMs use consumer data for further training, while only mentioning this very subtly in their privacy policy. This makes room for a plethora of data security leaks on the cloud where this data is hosted and other data attacks such as prompt injection.
&lt;br&gt;
[D] by virtue of huge data and state-of-the-art compute requirements, it is only the firms with a rich history in tech and high levels of disposable income to spend, who can establish themselves in this area, making it discriminatorily hard for smaller firms to emerge in this field. This also means a geopolitical issue for 2nd and 3rd world countries being completely dependent upon the 1st world for such foundational models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;climate impact&lt;/strong&gt; &lt;br&gt;
[D] these foundational models require huge loads of compute requirements for pre-training. This means great amounts of energy requirements, so much so that companies like Microsoft have rented complete nuclear reactors to satisfy the energy hunger of their AI programmes. This move is completely counterintuitive to the sustainability efforts being done to reverse climate change.
&lt;br&gt;
[S] this tradeoff, considering the first set of harms [1] as well, is completely unjustified and will only lead humanity to its doom.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;low Levels of Development in the essential fields of Healthcare and Finance&lt;/strong&gt; &lt;br&gt;
[P] since these models require unrestricted access to data to be trained, coupled with the issue that healthcare and financial data being highly private, we haven&amp;rsquo;t seen any major breakthroughs in these areas. AI has immense potential to change the lives of millions provided there is enough sustainable development in these fields, but the only thing that is holding us back is access to highly private data which is a justified concern given the first set of issues [1].&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;with all these concerns in mind, I came across this interesting framework called Federated Learning. coupled with a very interesting encryption scheme - homomorphic encryption - it can solve all the above-listed problems.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;federated learning:&lt;/strong&gt; &lt;br&gt;
In the current paradigm, the data travels to the model, and the model is trained. But what if the model travels to the data for training? That is, there is no transfer of data, therefore no room for data leaks, privacy issues and greater accessibility for smaller companies without data to compete with big giants.
&lt;br&gt;
This also needs better innovation as the race now changes to creating novel architectures rather than winning with more data.
&lt;br&gt;
Additionally, now there is a greater variety of data available to train these models including personal data - healthcare, financial data. This means better foundation models and more innovation in these underdeveloped areas.
&lt;br&gt;
Also, it means that the model is trained on individual devices, which implies that the computational and energy requirements are divided across devices, thus offloading the burden on the central server.
&lt;br&gt;
&lt;br&gt;
&lt;img src=&#34;https://1rvinn.github.io/img/crude/fed/fed1.png&#34; alt=&#34;federated learning&#34;&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;homomorphic encryption:&lt;/strong&gt;
&lt;br&gt;
in the current paradigm, this is how data encryption looks like during LLM inference:
&lt;br&gt;
&lt;br&gt;
&lt;img src=&#34;https://1rvinn.github.io/img/crude/fed/fed2.png&#34; alt=&#34;homomorphic encryption&#34;&gt;
&lt;br&gt;
the inference is done on decrypted data, which leads to a loophole that can be exploited by cyber attackers. Not only this, but it can also lead to other security issues like the LLM learning about personal details and prompt injections as elaborated in [1].
&lt;br&gt;
homomorphic encryption prevents this by supporting calculations to be made on encrypted data itself, mitigating any data security threats.
&lt;br&gt;
additionally, being based upon lattice-based cryptography, it is resilient to the rise of quantum computers which pose a threat to existing encryption schemes like the RSA.
&lt;br&gt;
\&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;if you notice, I have segregated the above problems into three categories - [D]: Democratization of AI, [P]: Privacy, [S]: Sustainability.
&lt;br&gt;
&lt;br&gt;
these 3 issues are often always overlooked whenever there is a big innovation, but later, people realise the importance of working on these - the same happened in the case of Internet, with heavy spending on cybersecurity, sustainability and privacy being prevalent only these days.
&lt;br&gt;
&lt;br&gt;
the above two frameworks can solve all these issues, empowering even a small startup from Maharashtra in India to compete with OpenAI based out of San Francisco. I truly believe this could be a revolutionary step for democratising AI development and boosting security.&lt;/p&gt;&lt;/blockquote&gt;
</content>
    </item>
    
    <item>
      <title>browseher</title>
      <link>https://1rvinn.github.io/build/browser/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://1rvinn.github.io/build/browser/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;10/05/25&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;i dont know why but recently i’ve been getting this urge to build a browser of my own. something that’s bespoke to my needs - has ai inbuilt, is junk free and everything is the way i want it. yeah maybe if it turns out to be amazing, i may plan on launching it but that’s for me to worry about later.&lt;/p&gt;
&lt;p&gt;i found this &lt;a href=&#34;https://browser.engineering/&#34;&gt;primer&lt;/a&gt; which is in and out of what a browser entails and how to build it on your own.&lt;/p&gt;</description>
      <content>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;10/05/25&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;i dont know why but recently i’ve been getting this urge to build a browser of my own. something that’s bespoke to my needs - has ai inbuilt, is junk free and everything is the way i want it. yeah maybe if it turns out to be amazing, i may plan on launching it but that’s for me to worry about later.&lt;/p&gt;
&lt;p&gt;i found this &lt;a href=&#34;https://browser.engineering/&#34;&gt;primer&lt;/a&gt; which is in and out of what a browser entails and how to build it on your own.&lt;/p&gt;
&lt;p&gt;doing a bit of research, i have realised that it is kinda hard to do it all from scratch, not only because it is going to be very long but also because of the compute inefficiency of the product you’ll get out with - very laggy, sucking all the juice out your ram.&lt;/p&gt;
&lt;p&gt;i could build it on top of chromium, but that wont teach me a lot about browsers and networking.&lt;/p&gt;
&lt;p&gt;so i have decided to do neither and both at the same time. i’ll follow the primer, understand the basics right and then switch over to building over chromium, which i believe is also going to be imperative if i want to integrate ai into it — could be wrong, i dont really know.&lt;/p&gt;
&lt;p&gt;the best thing would still be to do it all from scratch, but let’s see where reading the primer leads me to and then make a decision likewise.&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;[update] 12/06/25&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;have built a very bare bones version that can display basic web pages (only text lmao not even html). ik, very far away from my final goal, but i have realised how hard it is to build a browser and definitely wouldn&amp;rsquo;t be able to make the one of my dreams from scratch - will have to capitalise on the chromium infra (now i understand the imp of open source). \
but the process is v.v. cool cuz im learning a hell lot about how the internet functions. it aint all a blackbox anymore.&lt;/p&gt;
&lt;hr&gt;
</content>
    </item>
    
  </channel>
</rss>
